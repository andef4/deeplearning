{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import mnist\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 20\n",
    "LEARING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class GaussianNoise(object):\n",
    "    \"\"\"\n",
    "    Add gaussian noise to a numpy.ndarray (H x W x C)\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, sigma, random_state=np.random):\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __call__(self, image):\n",
    "        row, col, ch = image.shape\n",
    "        gauss = self.random_state.normal(self.mean, self.sigma, (row, col, ch))\n",
    "        gauss = gauss.reshape(row, col, ch)\n",
    "        image += torch.from_numpy(gauss).float()\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    GaussianNoise(0.01, 0.001),\n",
    "])\n",
    "dataset = mnist.MNIST('./data/', train=True, download=True, transform=train_transforms)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# validation set\n",
    "validation_dataset = mnist.MNIST('./data/', train=False, download=True, transform=transforms.ToTensor())\n",
    "validation_loader = DataLoader(dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "# input_size\n",
    "data, _ = next(iter(loader))\n",
    "x_size = len(data[0][0][0])\n",
    "y_size = len(data[0][0])\n",
    "input_size = x_size * y_size  # flatten 28x28 tensor to 1x784 tensor\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 50\n",
    "\n",
    "class Model2Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(input_size, HIDDEN_SIZE)\n",
    "        self.h2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.h3 = nn.Linear(HIDDEN_SIZE, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.data.view(-1, input_size)\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.h2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.h3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 500\n",
    "\n",
    "class Model1Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(input_size, HIDDEN_SIZE)\n",
    "        self.h2 = nn.Linear(HIDDEN_SIZE, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.data.view(-1, input_size)\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.h2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 100\n",
    "\n",
    "class Model1LinearDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(input_size, HIDDEN_SIZE)\n",
    "        self.h2 = nn.Linear(HIDDEN_SIZE, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.data.view(-1, input_size)\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.h2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv1(nn.Module):\n",
    "    def __init__(self, kernel_size=5, conv_out_channels=5, linear_size=50):\n",
    "        super().__init__()\n",
    "        if kernel_size % 2 != 1:\n",
    "            raise Exception('Only odd kernel_size are supported')\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "        self.conv1 = nn.Conv2d(1, conv_out_channels, kernel_size=kernel_size)\n",
    "        # convolution kernels are not applied on the border of the image, because the kernel would be outside the image\n",
    "        conv_layer_output_size = int(x_size - (kernel_size - 1))\n",
    "        self.pooled_pixels = int(conv_layer_output_size / 2)\n",
    "        self.h1 = nn.Linear(self.pooled_pixels * self.pooled_pixels  * conv_out_channels, linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, self.pooled_pixels * self.pooled_pixels * self.conv_out_channels)\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.h2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv2(nn.Module):\n",
    "    def __init__(self, kernel_size=5, conv_out_channels1=5, conv_out_channels2=5, linear_size=50):\n",
    "        super().__init__()\n",
    "        if kernel_size % 2 != 1:\n",
    "            raise Exception('Only odd kernel_size are supported')\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, conv_out_channels1, kernel_size=kernel_size)\n",
    "        conv_out_channels1 = conv_out_channels1\n",
    "        conv_layer_output_size1 = int(x_size - (kernel_size - 1))\n",
    "        pooled_pixels1 = int(conv_layer_output_size1 / 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(conv_out_channels1, conv_out_channels2, kernel_size=kernel_size)\n",
    "        self.conv_out_channels2 = conv_out_channels2\n",
    "        conv_layer_output_size2 = int(pooled_pixels1 - (kernel_size - 1))\n",
    "        self.pooled_pixels2 = int(conv_layer_output_size2 / 2)\n",
    "\n",
    "        self.h1 = nn.Linear(self.pooled_pixels2 * self.pooled_pixels2  * conv_out_channels2, linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = x.view(-1, self.pooled_pixels2 * self.pooled_pixels2 * self.conv_out_channels2)\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.h2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalulate(model):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    for data, labels in validation_loader:\n",
    "        predictions_per_class = model(data.cuda())\n",
    "        _, highest_prediction_class = predictions_per_class.max(1)\n",
    "        loss += F.nll_loss(predictions_per_class, labels.cuda())\n",
    "    return loss/len(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def learn(model, epochs=30):\n",
    "    optimizer = Adam(params=model.parameters(), lr=LEARING_RATE)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for data, labels in loader:\n",
    "            predictions_per_class = model(data.cuda())\n",
    "            highest_prediction, highest_prediction_class = predictions_per_class.max(1)\n",
    "\n",
    "            # how good are we? compare output with the target classes\n",
    "            loss = F.nll_loss(predictions_per_class, labels.cuda())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss = total_loss/len(loader)\n",
    "        validation_loss = evalulate(model)\n",
    "        print(f'Epoch: {epoch}, Train Loss: {train_loss}, Validation Loss: {validation_loss.item()}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model1Linear().cuda()\n",
    "learn(model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel_size': 5, 'conv_out_channels': 5, 'linear_size': 500}\n",
      "Epoch: 0, Train Loss: -0.8230898158053557, Validation Loss: -0.9288526773452759\n",
      "Epoch: 1, Train Loss: -0.95513512301445, Validation Loss: -0.9639253616333008\n",
      "Epoch: 2, Train Loss: -0.9642195931871732, Validation Loss: -0.9621785283088684\n",
      "Epoch: 3, Train Loss: -0.9716175346374512, Validation Loss: -0.972173810005188\n",
      "Epoch: 4, Train Loss: -0.9739205879370372, Validation Loss: -0.9746850728988647\n",
      "Epoch: 5, Train Loss: -0.9769066201249759, Validation Loss: -0.9768523573875427\n",
      "Epoch: 6, Train Loss: -0.9779947476784389, Validation Loss: -0.9776179790496826\n",
      "Epoch: 7, Train Loss: -0.9787428175806999, Validation Loss: -0.9810363054275513\n",
      "Epoch: 8, Train Loss: -0.9806776974995931, Validation Loss: -0.982674241065979\n",
      "Epoch: 9, Train Loss: -0.9806296016772588, Validation Loss: -0.979028582572937\n",
      "Epoch: 10, Train Loss: -0.9812834273775418, Validation Loss: -0.9834086894989014\n",
      "Epoch: 11, Train Loss: -0.9818317886789639, Validation Loss: -0.9846426248550415\n",
      "Epoch: 12, Train Loss: -0.9819615630308787, Validation Loss: -0.9841049909591675\n",
      "Epoch: 13, Train Loss: -0.9830510034163793, Validation Loss: -0.9839239120483398\n",
      "Epoch: 14, Train Loss: -0.9827643472750982, Validation Loss: -0.9821622371673584\n",
      "Epoch: 15, Train Loss: -0.9830739662051201, Validation Loss: -0.9834690690040588\n",
      "Epoch: 16, Train Loss: -0.9839180916349093, Validation Loss: -0.9800923466682434\n",
      "Epoch: 17, Train Loss: -0.9843986667195956, Validation Loss: -0.9851604700088501\n",
      "Epoch: 18, Train Loss: -0.9840265243450801, Validation Loss: -0.9869173169136047\n",
      "Epoch: 19, Train Loss: -0.983935419857502, Validation Loss: -0.9851865768432617\n",
      "Epoch: 20, Train Loss: -0.9854974524577459, Validation Loss: -0.9841215014457703\n",
      "Epoch: 21, Train Loss: -0.9855653241475423, Validation Loss: -0.9861441850662231\n",
      "Epoch: 22, Train Loss: -0.9851710870464643, Validation Loss: -0.9867847561836243\n",
      "Epoch: 23, Train Loss: -0.9846136361757915, Validation Loss: -0.9861685037612915\n",
      "Epoch: 24, Train Loss: -0.9865515891114871, Validation Loss: -0.9883080124855042\n",
      "Epoch: 25, Train Loss: -0.986057307779789, Validation Loss: -0.9828671813011169\n",
      "Epoch: 26, Train Loss: -0.9865680795709292, Validation Loss: -0.9818952679634094\n",
      "Epoch: 27, Train Loss: -0.9859585315783819, Validation Loss: -0.9875121116638184\n",
      "Epoch: 28, Train Loss: -0.9865037126143773, Validation Loss: -0.9851884841918945\n",
      "Epoch: 29, Train Loss: -0.9862218231161436, Validation Loss: -0.9860969185829163\n",
      "Epoch: 30, Train Loss: -0.9861973019838333, Validation Loss: -0.9869404435157776\n",
      "Epoch: 31, Train Loss: -0.9862072831193606, Validation Loss: -0.9875938296318054\n",
      "Epoch: 32, Train Loss: -0.9869313069383303, Validation Loss: -0.9868078231811523\n",
      "Epoch: 33, Train Loss: -0.9869476281007131, Validation Loss: -0.9863888025283813\n",
      "Epoch: 34, Train Loss: -0.9871176946163177, Validation Loss: -0.9891070127487183\n",
      "Epoch: 35, Train Loss: -0.9883146755297979, Validation Loss: -0.9887369871139526\n",
      "Epoch: 36, Train Loss: -0.9870556691288948, Validation Loss: -0.9877220988273621\n",
      "Epoch: 37, Train Loss: -0.9874456134239833, Validation Loss: -0.9864457249641418\n",
      "Epoch: 38, Train Loss: -0.9875440967082977, Validation Loss: -0.9884682297706604\n",
      "Epoch: 39, Train Loss: -0.9861925596197446, Validation Loss: -0.9882122278213501\n",
      "Epoch: 40, Train Loss: -0.9869342586994171, Validation Loss: -0.9876269698143005\n",
      "Epoch: 41, Train Loss: -0.9873809223373731, Validation Loss: -0.9881028532981873\n",
      "Epoch: 42, Train Loss: -0.987888278901577, Validation Loss: -0.9859609007835388\n",
      "Epoch: 43, Train Loss: -0.9877228834430377, Validation Loss: -0.9876147508621216\n",
      "Epoch: 44, Train Loss: -0.9875307547251383, Validation Loss: -0.9897249937057495\n",
      "Epoch: 45, Train Loss: -0.9879130155046781, Validation Loss: -0.9846806526184082\n",
      "Epoch: 46, Train Loss: -0.9875146876970927, Validation Loss: -0.9858419299125671\n",
      "Epoch: 47, Train Loss: -0.9874653468132019, Validation Loss: -0.9897351861000061\n",
      "Epoch: 48, Train Loss: -0.987350157558918, Validation Loss: -0.9897000193595886\n",
      "Epoch: 49, Train Loss: -0.986953439950943, Validation Loss: -0.9887586832046509\n",
      "Epoch: 50, Train Loss: -0.9877766698996227, Validation Loss: -0.9881700873374939\n",
      "Epoch: 51, Train Loss: -0.9885730376640955, Validation Loss: -0.9874038696289062\n",
      "Epoch: 52, Train Loss: -0.9885084088047346, Validation Loss: -0.9897782206535339\n",
      "Epoch: 53, Train Loss: -0.988545294602712, Validation Loss: -0.9892523288726807\n",
      "Epoch: 54, Train Loss: -0.9886797075271606, Validation Loss: -0.9888269901275635\n",
      "Epoch: 55, Train Loss: -0.988523168126742, Validation Loss: -0.9876390099525452\n",
      "Epoch: 56, Train Loss: -0.9883309899568558, Validation Loss: -0.9873573184013367\n",
      "Epoch: 57, Train Loss: -0.9886388864517212, Validation Loss: -0.98899906873703\n",
      "Epoch: 58, Train Loss: -0.9882743859887123, Validation Loss: -0.9890734553337097\n",
      "Epoch: 59, Train Loss: -0.9887318979501725, Validation Loss: -0.9893689751625061\n",
      "Epoch: 60, Train Loss: -0.9890794072747231, Validation Loss: -0.9867150783538818\n",
      "Epoch: 61, Train Loss: -0.9880982984304428, Validation Loss: -0.9891270399093628\n",
      "Epoch: 62, Train Loss: -0.9884268719156584, Validation Loss: -0.9876545071601868\n",
      "Epoch: 63, Train Loss: -0.9881130704283714, Validation Loss: -0.9896025061607361\n",
      "Epoch: 64, Train Loss: -0.9885500601530075, Validation Loss: -0.9863151907920837\n",
      "Epoch: 65, Train Loss: -0.9890935690204302, Validation Loss: -0.9892130494117737\n",
      "Epoch: 66, Train Loss: -0.9888388807376226, Validation Loss: -0.9888196587562561\n",
      "Epoch: 67, Train Loss: -0.9873061470588048, Validation Loss: -0.988669753074646\n",
      "Epoch: 68, Train Loss: -0.9887409550746282, Validation Loss: -0.9894903898239136\n",
      "Epoch: 69, Train Loss: -0.9880654790600141, Validation Loss: -0.989650547504425\n",
      "Epoch: 70, Train Loss: -0.9875182711482048, Validation Loss: -0.988383948802948\n",
      "Epoch: 71, Train Loss: -0.9885518802603086, Validation Loss: -0.9892878532409668\n",
      "Epoch: 72, Train Loss: -0.9891117780208588, Validation Loss: -0.9893280863761902\n",
      "Epoch: 73, Train Loss: -0.987993543346723, Validation Loss: -0.9880859851837158\n",
      "Epoch: 74, Train Loss: -0.9879908358454704, Validation Loss: -0.9892147779464722\n",
      "Epoch: 75, Train Loss: -0.9889641195336978, Validation Loss: -0.987646222114563\n",
      "Epoch: 76, Train Loss: -0.9877399617036183, Validation Loss: -0.9885383248329163\n",
      "Epoch: 77, Train Loss: -0.9882919409473737, Validation Loss: -0.9896084070205688\n",
      "Epoch: 78, Train Loss: -0.9881602296829224, Validation Loss: -0.988490641117096\n",
      "Epoch: 79, Train Loss: -0.9877463201483091, Validation Loss: -0.9873956441879272\n",
      "Epoch: 80, Train Loss: -0.9888095860878626, Validation Loss: -0.9906054735183716\n",
      "Epoch: 81, Train Loss: -0.9891044513781866, Validation Loss: -0.9899928569793701\n",
      "Epoch: 82, Train Loss: -0.9894672361016273, Validation Loss: -0.9896253943443298\n",
      "Epoch: 83, Train Loss: -0.9894844651023547, Validation Loss: -0.9890270829200745\n",
      "Epoch: 84, Train Loss: -0.9887576316396396, Validation Loss: -0.9904899597167969\n",
      "Epoch: 85, Train Loss: -0.9891062378883362, Validation Loss: -0.9864702224731445\n",
      "Epoch: 86, Train Loss: -0.9886484983166058, Validation Loss: -0.9885456562042236\n",
      "Epoch: 87, Train Loss: -0.9888059193094572, Validation Loss: -0.9874251484870911\n",
      "Epoch: 88, Train Loss: -0.9892820449074109, Validation Loss: -0.989567756652832\n",
      "Epoch: 89, Train Loss: -0.9893194288015366, Validation Loss: -0.986524224281311\n",
      "Epoch: 90, Train Loss: -0.9892086084683737, Validation Loss: -0.9871293306350708\n",
      "Epoch: 91, Train Loss: -0.9887734537323316, Validation Loss: -0.9898708462715149\n",
      "Epoch: 92, Train Loss: -0.9893747480312983, Validation Loss: -0.9886474609375\n",
      "Epoch: 93, Train Loss: -0.9885500537355741, Validation Loss: -0.9899815917015076\n",
      "Epoch: 94, Train Loss: -0.9898986077904701, Validation Loss: -0.9894630312919617\n",
      "Epoch: 95, Train Loss: -0.9903085881670316, Validation Loss: -0.9902781248092651\n",
      "Epoch: 96, Train Loss: -0.9903494429787, Validation Loss: -0.99101722240448\n",
      "Epoch: 97, Train Loss: -0.9901311130523681, Validation Loss: -0.9904177188873291\n",
      "Epoch: 98, Train Loss: -0.9893199506998062, Validation Loss: -0.9895772337913513\n",
      "Epoch: 99, Train Loss: -0.9895786801576615, Validation Loss: -0.9889106154441833\n"
     ]
    }
   ],
   "source": [
    "# 1 convolution layer\n",
    "configs = [\n",
    "    #{'kernel_size': 5, 'conv_out_channels': 1, 'linear_size': 50},\n",
    "    {'kernel_size': 5, 'conv_out_channels': 5, 'linear_size': 500},\n",
    "    #{'kernel_size': 3, 'conv_out_channels': 2, 'linear_size': 500},\n",
    "    #{'kernel_size': 3, 'conv_out_channels': 2, 'linear_size': 300},\n",
    "    #{'kernel_size': 3, 'conv_out_channels': 2, 'linear_size': 200},\n",
    "    #{'kernel_size': 5, 'conv_out_channels': 2, 'linear_size': 50},\n",
    "    #{'kernel_size': 5, 'conv_out_channels': 5, 'linear_size': 50},\n",
    "    #{'kernel_size': 9, 'conv_out_channels': 5, 'linear_size': 500},\n",
    "    #{'kernel_size': 7, 'conv_out_channels': 5, 'linear_size': 500},\n",
    "    #{'kernel_size': 11, 'conv_out_channels': 5, 'linear_size': 500},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    torch.cuda.empty_cache()\n",
    "    model = ModelConv1(**config).cuda()\n",
    "    try:\n",
    "        learn(model, 100)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2 convolution layers\n",
    "configs = [\n",
    "    #{'kernel_size': 5, 'conv_out_channels1': 5, 'conv_out_channels2': 5, 'linear_size': 50},\n",
    "    #{'kernel_size': 5, 'conv_out_channels1': 5, 'conv_out_channels2': 5, 'linear_size': 300},\n",
    "    #{'kernel_size': 5, 'conv_out_channels1': 5, 'conv_out_channels2': 5, 'linear_size': 500},\n",
    "    \n",
    "    #{'kernel_size': 5, 'conv_out_channels1': 5, 'conv_out_channels2': 10, 'linear_size': 50},\n",
    "    {'kernel_size': 5, 'conv_out_channels1': 5, 'conv_out_channels2': 10, 'linear_size': 300},\n",
    "    #{'kernel_size': 5, 'conv_out_channels1': 5, 'conv_out_channels2': 10, 'linear_size': 500},\n",
    "    \n",
    "    #{'kernel_size': 5, 'conv_out_channels1': 10, 'conv_out_channels2': 10, 'linear_size': 50},\n",
    "    #{'kernel_size': 5, 'conv_out_channels1': 10, 'conv_out_channels2': 10, 'linear_size': 300},\n",
    "    #{'kernel_size': 5, 'conv_out_channels1': 10, 'conv_out_channels2': 10, 'linear_size': 500},\n",
    "    \n",
    "    #{'kernel_size': 5, 'conv_out_channels1': 10, 'conv_out_channels2': 20, 'linear_size': 50},\n",
    "    #{'kernel_size': 5, 'conv_out_channels1': 10, 'conv_out_channels2': 20, 'linear_size': 300},\n",
    "    #{'kernel_size': 5, 'conv_out_channels1': 10, 'conv_out_channels2': 20, 'linear_size': 500},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    torch.cuda.empty_cache()\n",
    "    model = ModelConv2(**config).cuda()\n",
    "    try:\n",
    "        learn(model, 100)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
