{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from scipy.io import wavfile\n",
    "import scipy.signal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132300\n",
      "262144.0\n",
      "65536.0\n"
     ]
    }
   ],
   "source": [
    "n = 44100 * 3\n",
    "print(n)\n",
    "y = np.floor(np.log2(n))\n",
    "print(np.power(2, y+1))\n",
    "print(np.power(2, y-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 2\n",
    "SONG_LENGTH_SECONDS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, directory, genres, add_dimension, downsample=None, noise=False):\n",
    "        self.directory = directory\n",
    "        self.files = []\n",
    "        self.downsample = downsample\n",
    "        self.add_dimension = add_dimension\n",
    "        self.noise = noise\n",
    "        for label, genre in enumerate(genres):\n",
    "            genre_path = os.path.join(directory, genre)\n",
    "            self.files.extend([(os.path.join(genre_path, f), label) for f in os.listdir(genre_path)])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        song, label = self.files[index]\n",
    "        rate, data = wavfile.read(f'{self.directory}/{song}')\n",
    "        \n",
    "        data = data[:44100*SONG_LENGTH_SECONDS]\n",
    "        \n",
    "        if self.downsample:\n",
    "            data = scipy.signal.resample(data, self.downsample * SONG_LENGTH_SECONDS)\n",
    "\n",
    "        if self.noise:\n",
    "            gauss = np.random.normal(0.01, 0.001, (len(data),))\n",
    "            data = data + gauss\n",
    "        \n",
    "        tensor = torch.Tensor(data) / (1 << 31)\n",
    "        # add an input dimension to the data [441000] => [1, 441000]. Conv1d expects data in this format.\n",
    "        if self.add_dimension:\n",
    "            tensor.unsqueeze_(0)\n",
    "        return tensor, torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "    def input_size(self):\n",
    "        if self.add_dimension:\n",
    "            return len(self[0][0][0])\n",
    "        else:\n",
    "            return len(self[0][0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(add_dimension, noise=False):\n",
    "    d = MusicDataset('.', ['rock', 'electro', 'classic'], add_dimension, downsample=8000, noise=noise)\n",
    "    train, validate = random_split(d, [900, 300])\n",
    "\n",
    "    loader = DataLoader(train, batch_size=BATCH_SIZE)\n",
    "    validation_loader = DataLoader(validate, batch_size=BATCH_SIZE)\n",
    "    return d.input_size(), loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1Linear(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(input_size, hidden_size)\n",
    "        self.h2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h6 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h7 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h8 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h9 = nn.Linear(hidden_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.data.view(-1, input_size)\n",
    "         \n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h5(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h7(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h8(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h9(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv1(nn.Module):\n",
    "    def __init__(self, input_size, kernel_size=5, conv_out_channels=5, linear_size=50):\n",
    "        super().__init__()\n",
    "\n",
    "        if kernel_size % 2 != 1:\n",
    "            raise Exception('Only odd kernel_size are supported')\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "        self.conv1 = nn.Conv1d(1, conv_out_channels, kernel_size=kernel_size)\n",
    "        conv_layer_output_size = int(input_size - (kernel_size - 1))\n",
    "\n",
    "        self.pooled_samples = int(conv_layer_output_size / 2)\n",
    "        self.h1 = nn.Linear(self.pooled_samples * conv_out_channels, linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, linear_size)\n",
    "        self.h3 = nn.Linear(linear_size, linear_size)\n",
    "        self.h4 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h5 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h6 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h7 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h8 = nn.Linear(linear_size, linear_size)\n",
    "        self.h9 = nn.Linear(linear_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        x = x.view(-1, self.pooled_samples * self.conv_out_channels)\n",
    "\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.h2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.h3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.h4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.h9(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv2(nn.Module):\n",
    "    def __init__(self, input_size, kernel_size=5, conv_out_channels=5, linear_size=50):\n",
    "        super().__init__()\n",
    "\n",
    "        if kernel_size % 2 != 1:\n",
    "            raise Exception('Only odd kernel_size are supported')\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "        self.conv1 = nn.Conv1d(1, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv1d(conv_out_channels, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.conv3 = nn.Conv1d(conv_out_channels, conv_out_channels, kernel_size=kernel_size)\n",
    "\n",
    "        #conv_layer_output_size = int(input_size - (kernel_size - 1))\n",
    "        x = input_size\n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        \n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        \n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        self.pooled_samples = x * conv_out_channels\n",
    "\n",
    "        self.h1 = nn.Linear(self.pooled_samples, linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, linear_size)\n",
    "        self.h3 = nn.Linear(linear_size, linear_size)\n",
    "        self.h4 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h5 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h6 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h7 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h8 = nn.Linear(linear_size, linear_size)\n",
    "        self.h9 = nn.Linear(linear_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "\n",
    "        x = x.view(BATCH_SIZE, self.pooled_samples)\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.h2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.h3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.h4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.h9(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def evalulate(model, validation_loader):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    for data, labels in validation_loader:\n",
    "        predictions_per_class = model(data.cuda())\n",
    "        _, highest_prediction_class = predictions_per_class.max(1)\n",
    "        loss += F.nll_loss(predictions_per_class, labels.cuda())\n",
    "    return loss/len(validation_loader)\n",
    "\n",
    "def learn(model, loader, validation_loader, epochs=30, learning_rate=0.001):\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    f = open(f'{datetime.now().isoformat()}.txt', 'w', buffering=1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for data, labels in loader:\n",
    "            predictions_per_class = model(data.cuda())\n",
    "            highest_prediction, highest_prediction_class = predictions_per_class.max(1)\n",
    "\n",
    "            # how good are we? compare output with the target classes\n",
    "            loss = F.nll_loss(predictions_per_class, labels.cuda())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss = total_loss/len(loader)\n",
    "        validation_loss = evalulate(model, validation_loader)\n",
    "        stats = f'Epoch: {epoch}, Train Loss: {train_loss}, Validation Loss: {validation_loss.item()}'\n",
    "        print(stats)\n",
    "        f.write(f'{stats}\\n')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#input_size, loader, validation_loader = load_dataset(add_dimension=False, noise=False)\n",
    "#model = Model1Linear(input_size, 500).cuda()\n",
    "#learn(model, loader, validation_loader, 10000000, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: -0.33312338173389433, Validation Loss: -0.3316122591495514\n",
      "Epoch: 1, Train Loss: -0.33348923279179465, Validation Loss: -0.33080732822418213\n",
      "Epoch: 2, Train Loss: -0.33372972230116527, Validation Loss: -0.3300744593143463\n",
      "Epoch: 3, Train Loss: -0.3339423974355062, Validation Loss: -0.32928282022476196\n",
      "Epoch: 4, Train Loss: -0.3341088421146075, Validation Loss: -0.3283595144748688\n",
      "Epoch: 5, Train Loss: -0.3343545456727346, Validation Loss: -0.3272918164730072\n",
      "Epoch: 6, Train Loss: -0.3346570161978404, Validation Loss: -0.3259407579898834\n",
      "Epoch: 7, Train Loss: -0.3349996511141459, Validation Loss: -0.3243347704410553\n",
      "Epoch: 8, Train Loss: -0.33537302765581345, Validation Loss: -0.3225441873073578\n",
      "Epoch: 9, Train Loss: -0.3358563030759493, Validation Loss: -0.3208085894584656\n",
      "Epoch: 10, Train Loss: -0.3363778223759598, Validation Loss: -0.31914564967155457\n",
      "Epoch: 11, Train Loss: -0.3368589619298776, Validation Loss: -0.31776008009910583\n",
      "Epoch: 12, Train Loss: -0.33725143247180517, Validation Loss: -0.3167093098163605\n",
      "Epoch: 13, Train Loss: -0.33755734282235306, Validation Loss: -0.3159438967704773\n",
      "Epoch: 14, Train Loss: -0.3377670140316089, Validation Loss: -0.31539762020111084\n",
      "Epoch: 15, Train Loss: -0.337953119099968, Validation Loss: -0.314992755651474\n",
      "Epoch: 16, Train Loss: -0.338125310594009, Validation Loss: -0.31425291299819946\n",
      "Epoch: 17, Train Loss: -0.3379839691685306, Validation Loss: -0.3144851326942444\n",
      "Epoch: 18, Train Loss: -0.3382841079226798, Validation Loss: -0.3142816126346588\n",
      "Epoch: 19, Train Loss: -0.3383658016845584, Validation Loss: -0.31412121653556824\n",
      "Epoch: 20, Train Loss: -0.3384355189361506, Validation Loss: -0.31399133801460266\n",
      "Epoch: 21, Train Loss: -0.3384952641961475, Validation Loss: -0.3138825595378876\n",
      "Epoch: 22, Train Loss: -0.33854825257013244, Validation Loss: -0.31379133462905884\n",
      "Epoch: 23, Train Loss: -0.3385970508534875, Validation Loss: -0.31370651721954346\n",
      "Epoch: 24, Train Loss: -0.33862590649475655, Validation Loss: -0.31364765763282776\n",
      "Epoch: 25, Train Loss: -0.33867255011780395, Validation Loss: -0.31358712911605835\n",
      "Epoch: 26, Train Loss: -0.3386990757048544, Validation Loss: -0.3135424554347992\n",
      "Epoch: 27, Train Loss: -0.3387329261770679, Validation Loss: -0.31350386142730713\n",
      "Epoch: 28, Train Loss: -0.3387640826124698, Validation Loss: -0.3134714961051941\n",
      "Epoch: 29, Train Loss: -0.3387961032986641, Validation Loss: -0.31343892216682434\n",
      "Epoch: 30, Train Loss: -0.33882452070299124, Validation Loss: -0.31340888142585754\n",
      "Epoch: 31, Train Loss: -0.3388423749556144, Validation Loss: -0.3133929371833801\n",
      "Epoch: 32, Train Loss: -0.33887361127469273, Validation Loss: -0.31337177753448486\n",
      "Epoch: 33, Train Loss: -0.3388973324507889, Validation Loss: -0.3133547008037567\n",
      "Epoch: 34, Train Loss: -0.3389216115884483, Validation Loss: -0.31333816051483154\n",
      "Epoch: 35, Train Loss: -0.33894316765065824, Validation Loss: -0.31332483887672424\n",
      "Epoch: 36, Train Loss: -0.33896542612670194, Validation Loss: -0.3133101761341095\n",
      "Epoch: 37, Train Loss: -0.33899380886306363, Validation Loss: -0.3132861852645874\n",
      "Epoch: 38, Train Loss: -0.33899183874846334, Validation Loss: -0.313275545835495\n",
      "Epoch: 39, Train Loss: -0.33900087391440237, Validation Loss: -0.3132776618003845\n",
      "Epoch: 40, Train Loss: -0.33903957380188837, Validation Loss: -0.31326544284820557\n",
      "Epoch: 41, Train Loss: -0.3390566743330823, Validation Loss: -0.3132558763027191\n",
      "Epoch: 42, Train Loss: -0.33907176641333436, Validation Loss: -0.3132479190826416\n",
      "Epoch: 43, Train Loss: -0.33912450046692455, Validation Loss: -0.3132067322731018\n",
      "Epoch: 44, Train Loss: -0.33905838512660313, Validation Loss: -0.3132181465625763\n",
      "Epoch: 45, Train Loss: -0.3390957574070328, Validation Loss: -0.3132157623767853\n",
      "Epoch: 46, Train Loss: -0.33911973687664915, Validation Loss: -0.31321296095848083\n",
      "Epoch: 47, Train Loss: -0.339139180449355, Validation Loss: -0.3132065534591675\n",
      "Epoch: 48, Train Loss: -0.339158876799047, Validation Loss: -0.31319597363471985\n",
      "Epoch: 49, Train Loss: -0.3391621606212316, Validation Loss: -0.3131943643093109\n",
      "Epoch: 50, Train Loss: -0.33918102769750275, Validation Loss: -0.31319132447242737\n",
      "Epoch: 51, Train Loss: -0.3391950801392603, Validation Loss: -0.313188374042511\n",
      "Epoch: 52, Train Loss: -0.3392100367017297, Validation Loss: -0.31318438053131104\n",
      "Epoch: 53, Train Loss: -0.339224936204652, Validation Loss: -0.31317585706710815\n",
      "Epoch: 54, Train Loss: -0.33923242838121953, Validation Loss: -0.3131706714630127\n",
      "Epoch: 55, Train Loss: -0.33924850901537057, Validation Loss: -0.31316065788269043\n",
      "Epoch: 56, Train Loss: -0.339246786538408, Validation Loss: -0.31316494941711426\n",
      "Epoch: 57, Train Loss: -0.33926666625361473, Validation Loss: -0.3131638467311859\n"
     ]
    }
   ],
   "source": [
    "input_size, loader, validation_loader = load_dataset(add_dimension=True)\n",
    "model = ModelConv2(input_size, kernel_size=5, conv_out_channels=10, linear_size=500).cuda()\n",
    "learn(model, loader, validation_loader, epochs=10000, learning_rate=0.0001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
