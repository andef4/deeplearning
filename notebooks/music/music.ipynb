{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from scipy.io import wavfile\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 1\n",
    "SONG_LENGTH_SECONDS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, directory, genres, add_dimension, downsample=None, noise=False):\n",
    "        self.directory = directory\n",
    "        self.files = []\n",
    "        self.downsample = downsample\n",
    "        self.add_dimension = add_dimension\n",
    "        self.noise = noise\n",
    "        for label, genre in enumerate(genres):\n",
    "            genre_path = os.path.join(directory, genre)\n",
    "            self.files.extend([(os.path.join(genre_path, f), label) for f in os.listdir(genre_path)])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        song, label = self.files[index]\n",
    "        rate, data = wavfile.read(f'{self.directory}/{song}')\n",
    "        \n",
    "        data = data[:44100*SONG_LENGTH_SECONDS]\n",
    "        \n",
    "        if self.downsample:\n",
    "            data = scipy.signal.resample(data, self.downsample * SONG_LENGTH_SECONDS)\n",
    "\n",
    "        if self.noise:\n",
    "            gauss = np.random.normal(0.01, 0.001, (len(data),))\n",
    "            data = data + gauss\n",
    "        \n",
    "        tensor = torch.Tensor(data) / (2**15)\n",
    "        # add an input dimension to the data [441000] => [1, 441000]. Conv1d expects data in this format.\n",
    "        if self.add_dimension:\n",
    "            tensor.unsqueeze_(0)\n",
    "        return tensor, torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "    def input_size(self):\n",
    "        if self.add_dimension:\n",
    "            return len(self[0][0][0])\n",
    "        else:\n",
    "            return len(self[0][0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(add_dimension, downsample=None, noise=False):\n",
    "    d = MusicDataset('.', ['rock', 'electro', 'classic'], add_dimension, downsample=downsample, noise=noise)\n",
    "    train, validate = random_split(d, [900, 300])\n",
    "\n",
    "    loader = DataLoader(train, batch_size=BATCH_SIZE)\n",
    "    validation_loader = DataLoader(validate, batch_size=BATCH_SIZE)\n",
    "    return d.input_size(), loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1Linear(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(input_size, hidden_size)\n",
    "        self.h2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h6 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h7 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h8 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h9 = nn.Linear(hidden_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.data.view(-1, input_size)\n",
    "         \n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h5(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h7(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h8(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h9(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv1(nn.Module):\n",
    "    def __init__(self, input_size, kernel_size=5, conv_out_channels=5, linear_size=50):\n",
    "        super().__init__()\n",
    "\n",
    "        if kernel_size % 2 != 1:\n",
    "            raise Exception('Only odd kernel_size are supported')\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "        self.conv1 = nn.Conv1d(1, conv_out_channels, kernel_size=kernel_size)\n",
    "        conv_layer_output_size = int(input_size - (kernel_size - 1))\n",
    "\n",
    "        self.pooled_samples = int(conv_layer_output_size / 2)\n",
    "        self.h1 = nn.Linear(self.pooled_samples * conv_out_channels, linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, linear_size)\n",
    "        self.h3 = nn.Linear(linear_size, linear_size)\n",
    "        self.h4 = nn.Linear(linear_size, linear_size)\n",
    "        self.h9 = nn.Linear(linear_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        x = x.view(-1, self.pooled_samples * self.conv_out_channels)\n",
    "\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.h2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.h3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.h4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.h9(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv2(nn.Module):\n",
    "    def __init__(self, input_size, kernel_size=5, conv_out_channels=5, linear_size=50):\n",
    "        super().__init__()\n",
    "\n",
    "        if kernel_size % 2 != 1:\n",
    "            raise Exception('Only odd kernel_size are supported')\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "        self.conv1 = nn.Conv1d(1, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv1d(conv_out_channels, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.conv3 = nn.Conv1d(conv_out_channels, conv_out_channels, kernel_size=kernel_size)\n",
    "\n",
    "        #conv_layer_output_size = int(input_size - (kernel_size - 1))\n",
    "        x = input_size\n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        \n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        \n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        self.pooled_samples = x * conv_out_channels\n",
    "\n",
    "        self.h1 = nn.Linear(self.pooled_samples, linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, linear_size)\n",
    "        self.h3 = nn.Linear(linear_size, linear_size)\n",
    "        self.h4 = nn.Linear(linear_size, linear_size)\n",
    "        self.h9 = nn.Linear(linear_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "\n",
    "        x = x.view(BATCH_SIZE, self.pooled_samples)\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h9(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv2BatchNorm(nn.Module):\n",
    "    def __init__(self, input_size, kernel_size=5, conv_out_channels=5, linear_size=50):\n",
    "        super().__init__()\n",
    "\n",
    "        if kernel_size % 2 != 1:\n",
    "            raise Exception('Only odd kernel_size are supported')\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "        self.conv1 = nn.Conv1d(1, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(conv_out_channels)\n",
    "        self.conv2 = nn.Conv1d(conv_out_channels, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(conv_out_channels)\n",
    "        self.conv3 = nn.Conv1d(conv_out_channels, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(conv_out_channels)\n",
    "\n",
    "        #conv_layer_output_size = int(input_size - (kernel_size - 1))\n",
    "        x = input_size\n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        \n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        \n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        self.pooled_samples = x * conv_out_channels\n",
    "\n",
    "        self.h1 = nn.Linear(self.pooled_samples, linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, linear_size)\n",
    "        self.h3 = nn.Linear(linear_size, linear_size)\n",
    "        self.h4 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h5 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h6 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h7 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h8 = nn.Linear(linear_size, linear_size)\n",
    "        self.h9 = nn.Linear(linear_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "\n",
    "        x = x.view(BATCH_SIZE, self.pooled_samples)\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h9(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv3(nn.Module):\n",
    "    def __init__(self, input_size, kernel_size=5, conv_out_channels=5, linear_size=50):\n",
    "        super().__init__()\n",
    "\n",
    "        if kernel_size % 2 != 1:\n",
    "            raise Exception('Only odd kernel_size are supported')\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "        self.conv1 = nn.Conv1d(1, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv1d(conv_out_channels, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.conv3 = nn.Conv1d(conv_out_channels, conv_out_channels, kernel_size=kernel_size)\n",
    "\n",
    "        #conv_layer_output_size = int(input_size - (kernel_size - 1))\n",
    "        x = input_size\n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        \n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        \n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        self.pooled_samples = x * conv_out_channels\n",
    "\n",
    "        self.h1 = nn.Linear(self.pooled_samples, linear_size)\n",
    "        self.h9 = nn.Linear(linear_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "\n",
    "        x = x.view(BATCH_SIZE, self.pooled_samples)\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h9(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, device, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    f = open(f'{datetime.now().isoformat()}.txt', 'w', buffering=1)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = None\n",
    "        train_accuracy = None\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss = epoch_loss\n",
    "                train_accuracy = epoch_acc\n",
    "            else:\n",
    "                stats = f'Epoch: {epoch}, TL: {train_loss:.5f}, VL: {epoch_loss:.5f}, TA: {train_accuracy:.5f}, VA: {epoch_acc:.5f}'\n",
    "                print(stats)\n",
    "                f.write(f'{stats}\\n')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), f'{best_acc:.5f}.pth')\n",
    "                \n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size, loader, validation_loader = load_dataset(add_dimension=True, downsample=22050, noise=True)\n",
    "model = ModelConv2(input_size, kernel_size=5, conv_out_channels=10, linear_size=500).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(params=model.parameters(), lr=0.0001)\n",
    "train_model(model, {'train': loader, 'val': validation_loader}, criterion, optimizer, device, num_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#input_size, loader, validation_loader = load_dataset(add_dimension=False, noise=False)\n",
    "#model = Model1Linear(input_size, 500).cuda()\n",
    "#learn(model, loader, validation_loader, 10000, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#input_size, loader, validation_loader = load_dataset(add_dimension=True, noise=True)\n",
    "#model = ModelConv3(input_size, kernel_size=5, conv_out_channels=10, linear_size=500).cuda()\n",
    "#learn(model, loader, validation_loader, epochs=10000, learning_rate=0.0001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
