{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from scipy.io import wavfile\n",
    "import scipy.signal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 1\n",
    "SONG_LENGTH_SECONDS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, directory, genres, add_dimension, downsample=None, noise=False):\n",
    "        self.directory = directory\n",
    "        self.files = []\n",
    "        self.downsample = downsample\n",
    "        self.add_dimension = add_dimension\n",
    "        self.noise = noise\n",
    "        for label, genre in enumerate(genres):\n",
    "            genre_path = os.path.join(directory, genre)\n",
    "            self.files.extend([(os.path.join(genre_path, f), label) for f in os.listdir(genre_path)])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        song, label = self.files[index]\n",
    "        rate, data = wavfile.read(f'{self.directory}/{song}')\n",
    "        \n",
    "        data = data[:44100*SONG_LENGTH_SECONDS]\n",
    "        \n",
    "        if self.downsample:\n",
    "            data = scipy.signal.resample(data, self.downsample * SONG_LENGTH_SECONDS)\n",
    "\n",
    "        if self.noise:\n",
    "            gauss = np.random.normal(0.01, 0.001, (len(data),))\n",
    "            data = data + gauss\n",
    "        \n",
    "        tensor = torch.Tensor(data) / (2**15)\n",
    "        # add an input dimension to the data [441000] => [1, 441000]. Conv1d expects data in this format.\n",
    "        if self.add_dimension:\n",
    "            tensor.unsqueeze_(0)\n",
    "        return tensor, torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "    def input_size(self):\n",
    "        if self.add_dimension:\n",
    "            return len(self[0][0][0])\n",
    "        else:\n",
    "            return len(self[0][0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(add_dimension, downsample=None, noise=False):\n",
    "    d = MusicDataset('.', ['rock', 'electro', 'classic'], add_dimension, downsample=downsample, noise=noise)\n",
    "    train, validate = random_split(d, [900, 300])\n",
    "\n",
    "    loader = DataLoader(train, batch_size=BATCH_SIZE)\n",
    "    validation_loader = DataLoader(validate, batch_size=BATCH_SIZE)\n",
    "    return d.input_size(), loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1Linear(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(input_size, hidden_size)\n",
    "        self.h2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h6 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h7 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h8 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h9 = nn.Linear(hidden_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.data.view(-1, input_size)\n",
    "         \n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h5(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h7(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h8(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h9(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv1(nn.Module):\n",
    "    def __init__(self, input_size, kernel_size=5, conv_out_channels=5, linear_size=50):\n",
    "        super().__init__()\n",
    "\n",
    "        if kernel_size % 2 != 1:\n",
    "            raise Exception('Only odd kernel_size are supported')\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "        self.conv1 = nn.Conv1d(1, conv_out_channels, kernel_size=kernel_size)\n",
    "        conv_layer_output_size = int(input_size - (kernel_size - 1))\n",
    "\n",
    "        self.pooled_samples = int(conv_layer_output_size / 2)\n",
    "        self.h1 = nn.Linear(self.pooled_samples * conv_out_channels, linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, linear_size)\n",
    "        self.h3 = nn.Linear(linear_size, linear_size)\n",
    "        self.h4 = nn.Linear(linear_size, linear_size)\n",
    "        self.h9 = nn.Linear(linear_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        x = x.view(-1, self.pooled_samples * self.conv_out_channels)\n",
    "\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.h2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.h3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.h4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.h9(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv2(nn.Module):\n",
    "    def __init__(self, input_size, kernel_size=5, conv_out_channels=5, linear_size=50):\n",
    "        super().__init__()\n",
    "\n",
    "        if kernel_size % 2 != 1:\n",
    "            raise Exception('Only odd kernel_size are supported')\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "        self.conv1 = nn.Conv1d(1, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv1d(conv_out_channels, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.conv3 = nn.Conv1d(conv_out_channels, conv_out_channels, kernel_size=kernel_size)\n",
    "\n",
    "        #conv_layer_output_size = int(input_size - (kernel_size - 1))\n",
    "        x = input_size\n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        \n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        \n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        self.pooled_samples = x * conv_out_channels\n",
    "\n",
    "        self.h1 = nn.Linear(self.pooled_samples, linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, linear_size)\n",
    "        self.h3 = nn.Linear(linear_size, linear_size)\n",
    "        self.h4 = nn.Linear(linear_size, linear_size)\n",
    "        self.h9 = nn.Linear(linear_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "\n",
    "        x = x.view(BATCH_SIZE, self.pooled_samples)\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h9(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv2BatchNorm(nn.Module):\n",
    "    def __init__(self, input_size, kernel_size=5, conv_out_channels=5, linear_size=50):\n",
    "        super().__init__()\n",
    "\n",
    "        if kernel_size % 2 != 1:\n",
    "            raise Exception('Only odd kernel_size are supported')\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "        self.conv1 = nn.Conv1d(1, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(conv_out_channels)\n",
    "        self.conv2 = nn.Conv1d(conv_out_channels, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(conv_out_channels)\n",
    "        self.conv3 = nn.Conv1d(conv_out_channels, conv_out_channels, kernel_size=kernel_size)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(conv_out_channels)\n",
    "\n",
    "        #conv_layer_output_size = int(input_size - (kernel_size - 1))\n",
    "        x = input_size\n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        \n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        \n",
    "        x = x - (kernel_size - 1)\n",
    "        x = int(x / 5)\n",
    "        self.pooled_samples = x * conv_out_channels\n",
    "\n",
    "        self.h1 = nn.Linear(self.pooled_samples, linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, linear_size)\n",
    "        self.h3 = nn.Linear(linear_size, linear_size)\n",
    "        self.h4 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h5 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h6 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h7 = nn.Linear(linear_size, linear_size)\n",
    "        #self.h8 = nn.Linear(linear_size, linear_size)\n",
    "        self.h9 = nn.Linear(linear_size, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 5)\n",
    "\n",
    "        x = x.view(BATCH_SIZE, self.pooled_samples)\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.h4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.h9(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def evalulate(model, validation_loader):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    for data, labels in validation_loader:\n",
    "        labels = labels.cuda()\n",
    "        predictions_per_class = model(data.cuda())\n",
    "        _, highest_prediction_class = predictions_per_class.max(1)\n",
    "        loss += F.nll_loss(predictions_per_class, labels)\n",
    "        correct += torch.sum(highest_prediction_class == labels)\n",
    "    return loss/len(validation_loader), correct.item()/len(validation_loader)\n",
    "\n",
    "def learn(model, loader, validation_loader, epochs=30, learning_rate=0.001):\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    f = open(f'{datetime.now().isoformat()}.txt', 'w', buffering=1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        for data, labels in loader:\n",
    "            labels = labels.cuda()\n",
    "            predictions_per_class = model(data.cuda())\n",
    "            highest_prediction, highest_prediction_class = predictions_per_class.max(1)\n",
    "\n",
    "            # how good are we? compare output with the target classes\n",
    "            loss = F.nll_loss(predictions_per_class, labels)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += torch.sum(highest_prediction_class == labels)\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        accuracy = total_correct.item()/len(loader)\n",
    "        train_loss = total_loss/len(loader)\n",
    "        validation_loss, validation_accuracy = evalulate(model, validation_loader)\n",
    "        stats = f'Epoch: {epoch}, TL: {train_loss}, VL: {validation_loss.item()}, TA: {accuracy}, VA: {validation_accuracy}'\n",
    "        print(stats)\n",
    "        f.write(f'{stats}\\n')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#input_size, loader, validation_loader = load_dataset(add_dimension=False, noise=False)\n",
    "#model = Model1Linear(input_size, 500).cuda()\n",
    "#learn(model, loader, validation_loader, 10000, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, TL: -0.3649764043216904, VL: -0.4614359736442566, TA: 0.3522222222222222, VA: 0.64\n",
      "Epoch: 1, TL: -0.6230933185807319, VL: -0.7087326645851135, TA: 0.6322222222222222, VA: 0.7533333333333333\n",
      "Epoch: 2, TL: -0.686960410416758, VL: -0.7406435608863831, TA: 0.6966666666666667, VA: 0.74\n",
      "Epoch: 3, TL: -0.7380014009080672, VL: -0.7406772971153259, TA: 0.7444444444444445, VA: 0.74\n",
      "Epoch: 4, TL: -0.7407940070184649, VL: -0.7743238210678101, TA: 0.7477777777777778, VA: 0.7766666666666666\n",
      "Epoch: 5, TL: -0.7707625677954436, VL: -0.75943922996521, TA: 0.7722222222222223, VA: 0.7633333333333333\n",
      "Epoch: 6, TL: -0.7690609264652284, VL: -0.7727417349815369, TA: 0.7722222222222223, VA: 0.7733333333333333\n",
      "Epoch: 7, TL: -0.8005458617900321, VL: -0.747424304485321, TA: 0.8055555555555556, VA: 0.7466666666666667\n",
      "Epoch: 8, TL: -0.8067662760784916, VL: -0.7575620412826538, TA: 0.8077777777777778, VA: 0.7566666666666667\n",
      "Epoch: 9, TL: -0.8010284138400926, VL: -0.7590256333351135, TA: 0.8055555555555556, VA: 0.7566666666666667\n",
      "Epoch: 10, TL: -0.8327750479914171, VL: -0.7893308401107788, TA: 0.8366666666666667, VA: 0.7833333333333333\n",
      "Epoch: 11, TL: -0.8268278452528401, VL: -0.7660600543022156, TA: 0.8277777777777777, VA: 0.7633333333333333\n",
      "Epoch: 12, TL: -0.8304143971185746, VL: -0.8011093735694885, TA: 0.8333333333333334, VA: 0.8\n",
      "Epoch: 13, TL: -0.8452781948900323, VL: -0.8275967836380005, TA: 0.8477777777777777, VA: 0.83\n",
      "Epoch: 14, TL: -0.8436366891209219, VL: -0.8299630284309387, TA: 0.8444444444444444, VA: 0.83\n",
      "Epoch: 15, TL: -0.8590727940528871, VL: -0.802838146686554, TA: 0.86, VA: 0.8\n",
      "Epoch: 16, TL: -0.8168009213654337, VL: -0.827836275100708, TA: 0.8155555555555556, VA: 0.83\n",
      "Epoch: 17, TL: -0.8574223693699972, VL: -0.8203580379486084, TA: 0.8588888888888889, VA: 0.82\n",
      "Epoch: 18, TL: -0.8539250283132646, VL: -0.7631148099899292, TA: 0.8566666666666667, VA: 0.77\n",
      "Epoch: 19, TL: -0.8494717468966901, VL: -0.8285422921180725, TA: 0.8466666666666667, VA: 0.83\n",
      "Epoch: 20, TL: -0.8472973218448944, VL: -0.8358287215232849, TA: 0.8488888888888889, VA: 0.8366666666666667\n",
      "Epoch: 21, TL: -0.8678766564631712, VL: -0.8320653438568115, TA: 0.8677777777777778, VA: 0.83\n",
      "Epoch: 22, TL: -0.84611013848825, VL: -0.7931195497512817, TA: 0.8455555555555555, VA: 0.79\n",
      "Epoch: 23, TL: -0.8693283736635938, VL: -0.8269736170768738, TA: 0.8688888888888889, VA: 0.83\n",
      "Epoch: 24, TL: -0.8670209694779596, VL: -0.8124427199363708, TA: 0.8688888888888889, VA: 0.8066666666666666\n",
      "Epoch: 25, TL: -0.8672066147439328, VL: -0.835762619972229, TA: 0.8677777777777778, VA: 0.8333333333333334\n",
      "Epoch: 26, TL: -0.8627332269362114, VL: -0.8243969678878784, TA: 0.8633333333333333, VA: 0.8233333333333334\n",
      "Epoch: 27, TL: -0.8603824163002399, VL: -0.8387502431869507, TA: 0.86, VA: 0.84\n",
      "Epoch: 28, TL: -0.868173600544404, VL: -0.8058825135231018, TA: 0.8677777777777778, VA: 0.8033333333333333\n",
      "Epoch: 29, TL: -0.8609837551741797, VL: -0.8333193063735962, TA: 0.86, VA: 0.8333333333333334\n",
      "Epoch: 30, TL: -0.8782427865600807, VL: -0.8163602352142334, TA: 0.8788888888888889, VA: 0.8166666666666667\n",
      "Epoch: 31, TL: -0.881216323518676, VL: -0.8174687027931213, TA: 0.88, VA: 0.82\n",
      "Epoch: 32, TL: -0.876440602279653, VL: -0.8182583451271057, TA: 0.8766666666666667, VA: 0.82\n",
      "Epoch: 33, TL: -0.8966664283459704, VL: -0.8442893624305725, TA: 0.8988888888888888, VA: 0.8466666666666667\n",
      "Epoch: 34, TL: -0.884546515963988, VL: -0.8297046422958374, TA: 0.8844444444444445, VA: 0.83\n",
      "Epoch: 35, TL: -0.8941971704337683, VL: -0.8365450501441956, TA: 0.8944444444444445, VA: 0.8366666666666667\n",
      "Epoch: 36, TL: -0.8863899518517342, VL: -0.8362946510314941, TA: 0.8866666666666667, VA: 0.8366666666666667\n",
      "Epoch: 37, TL: -0.8886592988595465, VL: -0.8370131850242615, TA: 0.8888888888888888, VA: 0.84\n",
      "Epoch: 38, TL: -0.8750115612020255, VL: -0.805290162563324, TA: 0.8755555555555555, VA: 0.8033333333333333\n",
      "Epoch: 39, TL: -0.8765822629175779, VL: -0.8272919058799744, TA: 0.8777777777777778, VA: 0.8233333333333334\n",
      "Epoch: 40, TL: -0.8795968562587136, VL: -0.838268518447876, TA: 0.88, VA: 0.8366666666666667\n",
      "Epoch: 41, TL: -0.8992455713124545, VL: -0.8321646451950073, TA: 0.9011111111111111, VA: 0.83\n",
      "Epoch: 42, TL: -0.8924894182348181, VL: -0.832883358001709, TA: 0.8933333333333333, VA: 0.83\n",
      "Epoch: 43, TL: -0.8777695858544426, VL: -0.8359488248825073, TA: 0.8788888888888889, VA: 0.8366666666666667\n",
      "Epoch: 44, TL: -0.9074141411945605, VL: -0.8264602422714233, TA: 0.9077777777777778, VA: 0.8266666666666667\n",
      "Epoch: 45, TL: -0.8921063622687807, VL: -0.8335968852043152, TA: 0.8944444444444445, VA: 0.8333333333333334\n",
      "Epoch: 46, TL: -0.8961171365151135, VL: -0.8195917010307312, TA: 0.8966666666666666, VA: 0.82\n",
      "Epoch: 47, TL: -0.8969356551160278, VL: -0.8354283571243286, TA: 0.8966666666666666, VA: 0.8366666666666667\n",
      "Epoch: 48, TL: -0.9073331760861361, VL: -0.8367562294006348, TA: 0.9077777777777778, VA: 0.8333333333333334\n",
      "Epoch: 49, TL: -0.8463978084889158, VL: -0.8311718106269836, TA: 0.8466666666666667, VA: 0.83\n",
      "Epoch: 50, TL: -0.9075244683310123, VL: -0.8379093408584595, TA: 0.9088888888888889, VA: 0.8366666666666667\n",
      "Epoch: 51, TL: -0.903245345638953, VL: -0.8388077616691589, TA: 0.9044444444444445, VA: 0.8366666666666667\n",
      "Epoch: 52, TL: -0.9045218177934509, VL: -0.8208438158035278, TA: 0.9044444444444445, VA: 0.82\n",
      "Epoch: 53, TL: -0.9070750442144653, VL: -0.8253950476646423, TA: 0.9088888888888889, VA: 0.82\n",
      "Epoch: 54, TL: -0.9177344292138173, VL: -0.8472492694854736, TA: 0.9188888888888889, VA: 0.85\n",
      "Epoch: 55, TL: -0.9215795025216282, VL: -0.8378871083259583, TA: 0.9211111111111111, VA: 0.8333333333333334\n",
      "Epoch: 56, TL: -0.9261093103136656, VL: -0.861064076423645, TA: 0.9266666666666666, VA: 0.8633333333333333\n",
      "Epoch: 57, TL: -0.8933011130922617, VL: -0.8348967432975769, TA: 0.8944444444444445, VA: 0.8333333333333334\n",
      "Epoch: 58, TL: -0.9169470259052455, VL: -0.8334547281265259, TA: 0.9177777777777778, VA: 0.83\n",
      "Epoch: 59, TL: -0.9152598500808389, VL: -0.8277243375778198, TA: 0.9155555555555556, VA: 0.83\n",
      "Epoch: 60, TL: -0.9001901430659481, VL: -0.8362681865692139, TA: 0.8988888888888888, VA: 0.8333333333333334\n",
      "Epoch: 61, TL: -0.9191628240644779, VL: -0.8513144850730896, TA: 0.9177777777777778, VA: 0.85\n",
      "Epoch: 62, TL: -0.909934540329368, VL: -0.8311541676521301, TA: 0.91, VA: 0.8266666666666667\n",
      "Epoch: 63, TL: -0.9143974424229905, VL: -0.841842532157898, TA: 0.9133333333333333, VA: 0.8433333333333334\n",
      "Epoch: 64, TL: -0.9292299041591031, VL: -0.8486084938049316, TA: 0.93, VA: 0.8466666666666667\n",
      "Epoch: 65, TL: -0.9222810837647896, VL: -0.8505454659461975, TA: 0.9222222222222223, VA: 0.8466666666666667\n",
      "Epoch: 66, TL: -0.8918579891402046, VL: -0.8307479619979858, TA: 0.8933333333333333, VA: 0.83\n",
      "Epoch: 67, TL: -0.9071669802847735, VL: -0.8008508086204529, TA: 0.9077777777777778, VA: 0.7966666666666666\n",
      "Epoch: 68, TL: -0.8900345952597243, VL: -0.8228044509887695, TA: 0.8911111111111111, VA: 0.8233333333333334\n",
      "Epoch: 69, TL: -0.9207474469284597, VL: -0.8457022309303284, TA: 0.92, VA: 0.8466666666666667\n",
      "Epoch: 70, TL: -0.908670814153387, VL: -0.8439497947692871, TA: 0.9088888888888889, VA: 0.8466666666666667\n",
      "Epoch: 71, TL: -0.9073339710865054, VL: -0.8518528938293457, TA: 0.9077777777777778, VA: 0.85\n",
      "Epoch: 72, TL: -0.919571066074382, VL: -0.8488808274269104, TA: 0.92, VA: 0.8466666666666667\n",
      "Epoch: 73, TL: -0.8292142733475011, VL: -0.7228038907051086, TA: 0.83, VA: 0.7233333333333334\n",
      "Epoch: 74, TL: -0.9023190663872734, VL: -0.8591843843460083, TA: 0.9033333333333333, VA: 0.8633333333333333\n",
      "Epoch: 75, TL: -0.9147851115851738, VL: -0.8307942152023315, TA: 0.9155555555555556, VA: 0.83\n",
      "Epoch: 76, TL: -0.9320373755275604, VL: -0.8420811891555786, TA: 0.9322222222222222, VA: 0.84\n",
      "Epoch: 77, TL: -0.9221390578005944, VL: -0.8388636708259583, TA: 0.9211111111111111, VA: 0.8366666666666667\n",
      "Epoch: 78, TL: -0.933285654522955, VL: -0.8475826382637024, TA: 0.9333333333333333, VA: 0.8466666666666667\n",
      "Epoch: 79, TL: -0.9307000691820788, VL: -0.8450931310653687, TA: 0.93, VA: 0.8433333333333334\n",
      "Epoch: 80, TL: -0.9377218083790094, VL: -0.8570972681045532, TA: 0.9377777777777778, VA: 0.8566666666666667\n",
      "Epoch: 81, TL: -0.9017098238736184, VL: -0.8482665419578552, TA: 0.9011111111111111, VA: 0.8466666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82, TL: -0.9007519820001412, VL: -0.7655062675476074, TA: 0.9011111111111111, VA: 0.7666666666666667\n",
      "Epoch: 83, TL: -0.909693901619159, VL: -0.8455739617347717, TA: 0.91, VA: 0.8466666666666667\n",
      "Epoch: 84, TL: -0.9219435848167752, VL: -0.8537136316299438, TA: 0.9222222222222223, VA: 0.8533333333333334\n",
      "Epoch: 85, TL: -0.9109116214528837, VL: -0.8413394689559937, TA: 0.9111111111111111, VA: 0.8433333333333334\n",
      "Epoch: 86, TL: -0.5941891447834128, VL: -0.496910959482193, TA: 0.5944444444444444, VA: 0.49666666666666665\n",
      "Epoch: 87, TL: -0.8634561198220497, VL: -0.8454908728599548, TA: 0.8633333333333333, VA: 0.8433333333333334\n",
      "Epoch: 88, TL: -0.9267281784867735, VL: -0.8370979428291321, TA: 0.9266666666666666, VA: 0.8366666666666667\n",
      "Epoch: 89, TL: -0.9146617701213428, VL: -0.8607524037361145, TA: 0.9144444444444444, VA: 0.86\n",
      "Epoch: 90, TL: -0.9147202455406662, VL: -0.8460326790809631, TA: 0.9144444444444444, VA: 0.8466666666666667\n",
      "Epoch: 91, TL: -0.9354147641384927, VL: -0.7929368019104004, TA: 0.9366666666666666, VA: 0.7933333333333333\n",
      "Epoch: 92, TL: -0.8929577868734813, VL: -0.8323487043380737, TA: 0.8933333333333333, VA: 0.83\n",
      "Epoch: 93, TL: -0.9203339895900593, VL: -0.8482121229171753, TA: 0.92, VA: 0.8466666666666667\n",
      "Epoch: 94, TL: -0.9433213817366318, VL: -0.8485259413719177, TA: 0.9444444444444444, VA: 0.8466666666666667\n",
      "Epoch: 95, TL: -0.8485536763190789, VL: -0.8059139847755432, TA: 0.8488888888888889, VA: 0.8033333333333333\n",
      "Epoch: 96, TL: -0.9106090230728449, VL: -0.8379510045051575, TA: 0.91, VA: 0.8366666666666667\n",
      "Epoch: 97, TL: -0.934025648319291, VL: -0.8508881330490112, TA: 0.9344444444444444, VA: 0.85\n",
      "Epoch: 98, TL: -0.883131181984512, VL: -0.855250895023346, TA: 0.8844444444444445, VA: 0.8533333333333334\n",
      "Epoch: 99, TL: -0.935095321928219, VL: -0.8464528322219849, TA: 0.9344444444444444, VA: 0.8466666666666667\n",
      "Epoch: 100, TL: -0.873837064606022, VL: -0.8509712815284729, TA: 0.8733333333333333, VA: 0.85\n",
      "Epoch: 101, TL: -0.9317096480867427, VL: -0.8615542054176331, TA: 0.9311111111111111, VA: 0.8633333333333333\n",
      "Epoch: 102, TL: -0.9442047270558935, VL: -0.8525629639625549, TA: 0.9444444444444444, VA: 0.8533333333333334\n",
      "Epoch: 103, TL: -0.9415976065130469, VL: -0.8578669428825378, TA: 0.9411111111111111, VA: 0.8566666666666667\n",
      "Epoch: 104, TL: -0.9455441193486142, VL: -0.8554093837738037, TA: 0.9455555555555556, VA: 0.8533333333333334\n",
      "Epoch: 105, TL: -0.9247693355999183, VL: -0.840883195400238, TA: 0.9244444444444444, VA: 0.84\n",
      "Epoch: 106, TL: -0.9381877241535593, VL: -0.8092338442802429, TA: 0.9377777777777778, VA: 0.8066666666666666\n",
      "Epoch: 107, TL: -0.9308815502114433, VL: -0.849134087562561, TA: 0.9311111111111111, VA: 0.85\n",
      "Epoch: 108, TL: -0.9406395923523851, VL: -0.8516258597373962, TA: 0.9411111111111111, VA: 0.8533333333333334\n",
      "Epoch: 109, TL: -0.9381232106756419, VL: -0.8506994247436523, TA: 0.9377777777777778, VA: 0.85\n",
      "Epoch: 110, TL: -0.9456491520987719, VL: -0.8458683490753174, TA: 0.9455555555555556, VA: 0.85\n",
      "Epoch: 111, TL: -0.9363727131447481, VL: -0.8392860889434814, TA: 0.9355555555555556, VA: 0.84\n",
      "Epoch: 112, TL: -0.899526058581349, VL: -0.8167508840560913, TA: 0.8988888888888888, VA: 0.8166666666666667\n",
      "Epoch: 113, TL: -0.9094138921981672, VL: -0.85578453540802, TA: 0.9088888888888889, VA: 0.8533333333333334\n",
      "Epoch: 114, TL: -0.9410559701261612, VL: -0.8488123416900635, TA: 0.9411111111111111, VA: 0.85\n",
      "Epoch: 115, TL: -0.9392944204418184, VL: -0.8528977632522583, TA: 0.9388888888888889, VA: 0.8566666666666667\n",
      "Epoch: 116, TL: -0.9362228925800976, VL: -0.8395226001739502, TA: 0.9366666666666666, VA: 0.84\n",
      "Epoch: 117, TL: -0.9287997131978547, VL: -0.8247899413108826, TA: 0.9288888888888889, VA: 0.8266666666666667\n",
      "Epoch: 118, TL: -0.9326919690263443, VL: -0.837289571762085, TA: 0.9322222222222222, VA: 0.8366666666666667\n",
      "Epoch: 119, TL: -0.8789267798592756, VL: -0.81098473072052, TA: 0.8788888888888889, VA: 0.81\n",
      "Epoch: 120, TL: -0.9321084359240305, VL: -0.8380860686302185, TA: 0.9322222222222222, VA: 0.8366666666666667\n",
      "Epoch: 121, TL: -0.9317733803041112, VL: -0.8337831497192383, TA: 0.9322222222222222, VA: 0.8333333333333334\n",
      "Epoch: 122, TL: -0.935047775317685, VL: -0.8393110036849976, TA: 0.9344444444444444, VA: 0.84\n",
      "Epoch: 123, TL: -0.9491201587688562, VL: -0.8495805859565735, TA: 0.95, VA: 0.85\n",
      "Epoch: 124, TL: -0.9377564771356968, VL: -0.8467047810554504, TA: 0.9388888888888889, VA: 0.8466666666666667\n",
      "Epoch: 125, TL: -0.9522149514571159, VL: -0.8448978662490845, TA: 0.9522222222222222, VA: 0.8466666666666667\n",
      "Epoch: 126, TL: -0.9477414149224448, VL: -0.854799747467041, TA: 0.9477777777777778, VA: 0.8566666666666667\n",
      "Epoch: 127, TL: -0.9461855754086718, VL: -0.8264482021331787, TA: 0.9455555555555556, VA: 0.8266666666666667\n",
      "Epoch: 128, TL: -0.9266062079220732, VL: -0.8447074294090271, TA: 0.9266666666666666, VA: 0.8433333333333334\n",
      "Epoch: 129, TL: -0.9435489069185171, VL: -0.8473941683769226, TA: 0.9433333333333334, VA: 0.8466666666666667\n",
      "Epoch: 130, TL: -0.9299140696656704, VL: -0.8384048938751221, TA: 0.93, VA: 0.8366666666666667\n",
      "Epoch: 131, TL: -0.9346285033216912, VL: -0.8698065280914307, TA: 0.9355555555555556, VA: 0.87\n",
      "Epoch: 132, TL: -0.9499006535446909, VL: -0.855695903301239, TA: 0.95, VA: 0.8566666666666667\n",
      "Epoch: 133, TL: -0.9279224469274584, VL: -0.8460091948509216, TA: 0.9277777777777778, VA: 0.8466666666666667\n",
      "Epoch: 134, TL: -0.9285645700802686, VL: -0.8495714664459229, TA: 0.9288888888888889, VA: 0.8533333333333334\n",
      "Epoch: 135, TL: -0.9434665632318956, VL: -0.8501855134963989, TA: 0.9444444444444444, VA: 0.85\n",
      "Epoch: 136, TL: -0.9418101556923825, VL: -0.8537266254425049, TA: 0.9422222222222222, VA: 0.8533333333333334\n",
      "Epoch: 137, TL: -0.9113598041278584, VL: -0.8485006093978882, TA: 0.9122222222222223, VA: 0.85\n",
      "Epoch: 138, TL: -0.9451866779417103, VL: -0.8265741467475891, TA: 0.9455555555555556, VA: 0.8266666666666667\n",
      "Epoch: 139, TL: -0.9301040522504257, VL: -0.8170776963233948, TA: 0.93, VA: 0.8166666666666667\n",
      "Epoch: 140, TL: -0.941721770208754, VL: -0.8446633815765381, TA: 0.9422222222222222, VA: 0.8433333333333334\n",
      "Epoch: 141, TL: -0.9499529441217782, VL: -0.8489895462989807, TA: 0.95, VA: 0.85\n",
      "Epoch: 142, TL: -0.9488893643312171, VL: -0.8476194143295288, TA: 0.9488888888888889, VA: 0.8466666666666667\n",
      "Epoch: 143, TL: -0.9215095682376414, VL: -0.6889762282371521, TA: 0.9211111111111111, VA: 0.69\n",
      "Epoch: 144, TL: -0.92983330648792, VL: -0.866629958152771, TA: 0.93, VA: 0.8666666666666667\n",
      "Epoch: 145, TL: -0.9461720664662693, VL: -0.8615525364875793, TA: 0.9466666666666667, VA: 0.86\n",
      "Epoch: 146, TL: -0.9527251184518605, VL: -0.8622332811355591, TA: 0.9533333333333334, VA: 0.8633333333333333\n",
      "Epoch: 147, TL: -0.9248112711261386, VL: -0.8599172234535217, TA: 0.9244444444444444, VA: 0.86\n",
      "Epoch: 148, TL: -0.9451544878887126, VL: -0.848629355430603, TA: 0.9455555555555556, VA: 0.85\n",
      "Epoch: 149, TL: -0.9409306333470522, VL: -0.8641407489776611, TA: 0.9411111111111111, VA: 0.8633333333333333\n",
      "Epoch: 150, TL: -0.954310536099007, VL: -0.8625532984733582, TA: 0.9544444444444444, VA: 0.8633333333333333\n",
      "Epoch: 151, TL: -0.9521916811626849, VL: -0.8542686104774475, TA: 0.9522222222222222, VA: 0.8533333333333334\n",
      "Epoch: 152, TL: -0.9532924613235948, VL: -0.8580752015113831, TA: 0.9533333333333334, VA: 0.86\n",
      "Epoch: 153, TL: -0.9403004479093865, VL: -0.8536979556083679, TA: 0.94, VA: 0.8533333333333334\n",
      "Epoch: 154, TL: -0.9436595320477364, VL: -0.8696671724319458, TA: 0.9433333333333334, VA: 0.87\n",
      "Epoch: 155, TL: -0.9513991781614873, VL: -0.8546445965766907, TA: 0.9511111111111111, VA: 0.8533333333333334\n",
      "Epoch: 156, TL: -0.9511104673820504, VL: -0.8514211177825928, TA: 0.9511111111111111, VA: 0.85\n",
      "Epoch: 157, TL: -0.95555572014892, VL: -0.8504513502120972, TA: 0.9555555555555556, VA: 0.85\n",
      "Epoch: 158, TL: -0.9477554449874789, VL: -0.8247401714324951, TA: 0.9477777777777778, VA: 0.8233333333333334\n",
      "Epoch: 159, TL: -0.9441345705700191, VL: -0.8663069009780884, TA: 0.9444444444444444, VA: 0.8666666666666667\n",
      "Epoch: 160, TL: -0.9091924596121693, VL: -0.8535131216049194, TA: 0.9088888888888889, VA: 0.8533333333333334\n",
      "Epoch: 161, TL: -0.9069648387025739, VL: -0.7660294771194458, TA: 0.9066666666666666, VA: 0.7666666666666667\n",
      "Epoch: 162, TL: -0.9420231539114989, VL: -0.8567887544631958, TA: 0.9422222222222222, VA: 0.8566666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 163, TL: -0.9287210460765681, VL: -0.8366678953170776, TA: 0.9288888888888889, VA: 0.8366666666666667\n",
      "Epoch: 164, TL: -0.93645261405177, VL: -0.840001106262207, TA: 0.9366666666666666, VA: 0.84\n",
      "Epoch: 165, TL: -0.9268232147125367, VL: -0.8527682423591614, TA: 0.9266666666666666, VA: 0.8533333333333334\n",
      "Epoch: 166, TL: -0.9527531723254894, VL: -0.8486665487289429, TA: 0.9533333333333334, VA: 0.8466666666666667\n",
      "Epoch: 167, TL: -0.9490482899322119, VL: -0.8551359176635742, TA: 0.9488888888888889, VA: 0.8533333333333334\n",
      "Epoch: 168, TL: -0.9549675599642099, VL: -0.8470048904418945, TA: 0.9555555555555556, VA: 0.8466666666666667\n",
      "Epoch: 169, TL: -0.9544306262550328, VL: -0.8391308188438416, TA: 0.9544444444444444, VA: 0.8366666666666667\n",
      "Epoch: 170, TL: -0.9531985682625487, VL: -0.8550567626953125, TA: 0.9533333333333334, VA: 0.8566666666666667\n",
      "Epoch: 171, TL: -0.9181636585316941, VL: -0.7587472796440125, TA: 0.9188888888888889, VA: 0.76\n",
      "Epoch: 172, TL: -0.9407381905174607, VL: -0.8520156741142273, TA: 0.9411111111111111, VA: 0.8566666666666667\n",
      "Epoch: 173, TL: -0.9414286581870143, VL: -0.843708336353302, TA: 0.9411111111111111, VA: 0.8433333333333334\n",
      "Epoch: 174, TL: -0.9535176947175861, VL: -0.848821759223938, TA: 0.9533333333333334, VA: 0.85\n",
      "Epoch: 175, TL: -0.9529154434290725, VL: -0.8411407470703125, TA: 0.9533333333333334, VA: 0.84\n",
      "Epoch: 176, TL: -0.9328800014547849, VL: -0.7594728469848633, TA: 0.9333333333333333, VA: 0.76\n",
      "Epoch: 177, TL: -0.9427352839905788, VL: -0.8466596007347107, TA: 0.9433333333333334, VA: 0.8466666666666667\n",
      "Epoch: 178, TL: -0.9543892790015102, VL: -0.8368644714355469, TA: 0.9544444444444444, VA: 0.8366666666666667\n",
      "Epoch: 179, TL: -0.9337548619854883, VL: -0.8466630578041077, TA: 0.9333333333333333, VA: 0.8466666666666667\n",
      "Epoch: 180, TL: -0.9489616294959846, VL: -0.8531435132026672, TA: 0.9488888888888889, VA: 0.8533333333333334\n",
      "Epoch: 181, TL: -0.9534250260155921, VL: -0.8487963080406189, TA: 0.9533333333333334, VA: 0.8466666666666667\n",
      "Epoch: 182, TL: -0.9599652127431042, VL: -0.8261235356330872, TA: 0.96, VA: 0.8266666666666667\n",
      "Epoch: 183, TL: -0.9390694002998055, VL: -0.7898989915847778, TA: 0.9388888888888889, VA: 0.79\n",
      "Epoch: 184, TL: -0.9433468121073573, VL: -0.8332515954971313, TA: 0.9433333333333334, VA: 0.8333333333333334\n",
      "Epoch: 185, TL: -0.9510546778681667, VL: -0.8288992047309875, TA: 0.9511111111111111, VA: 0.83\n",
      "Epoch: 186, TL: -0.948653363150564, VL: -0.842552661895752, TA: 0.9477777777777778, VA: 0.8433333333333334\n",
      "Epoch: 187, TL: -0.9544456725404378, VL: -0.825124979019165, TA: 0.9544444444444444, VA: 0.8233333333333334\n",
      "Epoch: 188, TL: -0.8843140323850432, VL: -0.7243333458900452, TA: 0.8844444444444445, VA: 0.7233333333333334\n",
      "Epoch: 189, TL: -0.9061640081149214, VL: -0.8470636010169983, TA: 0.9066666666666666, VA: 0.8466666666666667\n",
      "Epoch: 190, TL: -0.9544444440867522, VL: -0.8470525741577148, TA: 0.9544444444444444, VA: 0.8466666666666667\n",
      "Epoch: 191, TL: -0.9544369831224058, VL: -0.8500234484672546, TA: 0.9544444444444444, VA: 0.85\n",
      "Epoch: 192, TL: -0.9177648185485657, VL: -0.8133323192596436, TA: 0.9177777777777778, VA: 0.8133333333333334\n",
      "Epoch: 193, TL: -0.9496889967050752, VL: -0.8486071228981018, TA: 0.95, VA: 0.85\n",
      "Epoch: 194, TL: -0.9499937949272547, VL: -0.8371521830558777, TA: 0.95, VA: 0.8366666666666667\n",
      "Epoch: 195, TL: -0.9353543080418862, VL: -0.8455705046653748, TA: 0.9355555555555556, VA: 0.8466666666666667\n",
      "Epoch: 196, TL: -0.9557438181888993, VL: -0.8453930616378784, TA: 0.9555555555555556, VA: 0.8433333333333334\n",
      "Epoch: 197, TL: -0.9554253007951372, VL: -0.8473700284957886, TA: 0.9555555555555556, VA: 0.8466666666666667\n",
      "Epoch: 198, TL: -0.9555551088518751, VL: -0.8474768996238708, TA: 0.9555555555555556, VA: 0.8466666666666667\n",
      "Epoch: 199, TL: -0.9566666173934941, VL: -0.8471379280090332, TA: 0.9566666666666667, VA: 0.8466666666666667\n",
      "Epoch: 200, TL: -0.9274485506564087, VL: -0.8495453000068665, TA: 0.9277777777777778, VA: 0.85\n",
      "Epoch: 201, TL: -0.9577638659895956, VL: -0.8552982807159424, TA: 0.9577777777777777, VA: 0.8533333333333334\n",
      "Epoch: 202, TL: -0.9515305340173849, VL: -0.8468644022941589, TA: 0.9511111111111111, VA: 0.8466666666666667\n",
      "Epoch: 203, TL: -0.9599572745098797, VL: -0.854366660118103, TA: 0.96, VA: 0.8566666666666667\n",
      "Epoch: 204, TL: -0.9498777763111522, VL: -0.8450593948364258, TA: 0.95, VA: 0.8433333333333334\n",
      "Epoch: 205, TL: -0.9398203940054468, VL: -0.8457801342010498, TA: 0.94, VA: 0.8466666666666667\n",
      "Epoch: 206, TL: -0.9555764021596168, VL: -0.8444318771362305, TA: 0.9555555555555556, VA: 0.8433333333333334\n",
      "Epoch: 207, TL: -0.9263952709827927, VL: -0.8063722848892212, TA: 0.9266666666666666, VA: 0.8066666666666666\n",
      "Epoch: 208, TL: -0.9336284112433595, VL: -0.8066702485084534, TA: 0.9333333333333333, VA: 0.8066666666666666\n",
      "Epoch: 209, TL: -0.952379689849196, VL: -0.8311248421669006, TA: 0.9522222222222222, VA: 0.83\n",
      "Epoch: 210, TL: -0.935611729986138, VL: -0.8167651891708374, TA: 0.9355555555555556, VA: 0.8166666666666667\n",
      "Epoch: 211, TL: -0.9323121373719491, VL: -0.7895784974098206, TA: 0.9322222222222222, VA: 0.79\n",
      "Epoch: 212, TL: -0.9399579037486832, VL: -0.8500008583068848, TA: 0.94, VA: 0.85\n",
      "Epoch: 213, TL: -0.9212698013432784, VL: -0.7928842902183533, TA: 0.9211111111111111, VA: 0.7933333333333333\n",
      "Epoch: 214, TL: -0.9285923046131644, VL: -0.8329861164093018, TA: 0.9288888888888889, VA: 0.8333333333333334\n",
      "Epoch: 215, TL: -0.9324968546867598, VL: -0.8433325290679932, TA: 0.9322222222222222, VA: 0.8433333333333334\n",
      "Epoch: 216, TL: -0.9535939161650809, VL: -0.8434188365936279, TA: 0.9544444444444444, VA: 0.8433333333333334\n",
      "Epoch: 217, TL: -0.940322034528911, VL: -0.7815486788749695, TA: 0.94, VA: 0.7833333333333333\n",
      "Epoch: 218, TL: -0.9498276308122634, VL: -0.8497030138969421, TA: 0.95, VA: 0.85\n",
      "Epoch: 219, TL: -0.9566666671872219, VL: -0.849853515625, TA: 0.9566666666666667, VA: 0.85\n",
      "Epoch: 220, TL: -0.9521717896726396, VL: -0.8366659283638, TA: 0.9522222222222222, VA: 0.8366666666666667\n",
      "Epoch: 221, TL: -0.9518305179116967, VL: -0.8466368913650513, TA: 0.9522222222222222, VA: 0.8466666666666667\n",
      "Epoch: 222, TL: -0.9510249224588433, VL: -0.8598968982696533, TA: 0.9511111111111111, VA: 0.86\n",
      "Epoch: 223, TL: -0.9098525729377808, VL: -0.8357127904891968, TA: 0.91, VA: 0.8366666666666667\n",
      "Epoch: 224, TL: -0.9401009062501711, VL: -0.844994843006134, TA: 0.94, VA: 0.8433333333333334\n",
      "Epoch: 225, TL: -0.9588843123118083, VL: -0.8458656072616577, TA: 0.9588888888888889, VA: 0.8433333333333334\n",
      "Epoch: 226, TL: -0.9500023330564651, VL: -0.8469791412353516, TA: 0.95, VA: 0.8466666666666667\n",
      "Epoch: 227, TL: -0.9555555579230118, VL: -0.8464613556861877, TA: 0.9555555555555556, VA: 0.8466666666666667\n",
      "Epoch: 228, TL: -0.9622208878737111, VL: -0.8463753461837769, TA: 0.9622222222222222, VA: 0.8466666666666667\n",
      "Epoch: 229, TL: -0.9259351099327333, VL: -0.8161130547523499, TA: 0.9255555555555556, VA: 0.8166666666666667\n",
      "Epoch: 230, TL: -0.9506534122203972, VL: -0.8350844383239746, TA: 0.9511111111111111, VA: 0.8366666666666667\n",
      "Epoch: 231, TL: -0.9572101977608637, VL: -0.8354723453521729, TA: 0.9577777777777777, VA: 0.8366666666666667\n",
      "Epoch: 232, TL: -0.9521018375724116, VL: -0.750254213809967, TA: 0.9522222222222222, VA: 0.75\n",
      "Epoch: 233, TL: -0.9256228074827765, VL: -0.8132908344268799, TA: 0.9255555555555556, VA: 0.8133333333333334\n",
      "Epoch: 234, TL: -0.9633307882812288, VL: -0.8402423858642578, TA: 0.9633333333333334, VA: 0.8433333333333334\n",
      "Epoch: 235, TL: -0.952162062653376, VL: -0.8511710166931152, TA: 0.9522222222222222, VA: 0.85\n",
      "Epoch: 236, TL: -0.949999873108334, VL: -0.8527316451072693, TA: 0.95, VA: 0.8533333333333334\n",
      "Epoch: 237, TL: -0.9566666666666667, VL: -0.8534266352653503, TA: 0.9566666666666667, VA: 0.8533333333333334\n",
      "Epoch: 238, TL: -0.9479359469532215, VL: -0.8466051816940308, TA: 0.9477777777777778, VA: 0.8466666666666667\n",
      "Epoch: 239, TL: -0.9599994189350539, VL: -0.8499943017959595, TA: 0.96, VA: 0.85\n",
      "Epoch: 240, TL: -0.9474902498598508, VL: -0.8302218914031982, TA: 0.9477777777777778, VA: 0.83\n",
      "Epoch: 241, TL: -0.9438131775979938, VL: -0.8432208895683289, TA: 0.9433333333333334, VA: 0.8433333333333334\n",
      "Epoch: 242, TL: -0.9476461104429917, VL: -0.8299705982208252, TA: 0.9477777777777778, VA: 0.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 243, TL: -0.9486573676600579, VL: -0.8149665594100952, TA: 0.9488888888888889, VA: 0.8133333333333334\n",
      "Epoch: 244, TL: -0.945485077397175, VL: -0.8424011468887329, TA: 0.9455555555555556, VA: 0.8433333333333334\n",
      "Epoch: 245, TL: -0.9381842186377845, VL: -0.7936537265777588, TA: 0.9377777777777778, VA: 0.7933333333333333\n",
      "Epoch: 246, TL: -0.9582747430085404, VL: -0.8433337211608887, TA: 0.9577777777777777, VA: 0.8433333333333334\n",
      "Epoch: 247, TL: -0.957843022168947, VL: -0.8420405387878418, TA: 0.9577777777777777, VA: 0.8433333333333334\n",
      "Epoch: 248, TL: -0.9599999982780949, VL: -0.8420293927192688, TA: 0.96, VA: 0.8433333333333334\n",
      "Epoch: 249, TL: -0.9611111175067006, VL: -0.8433529138565063, TA: 0.9611111111111111, VA: 0.8433333333333334\n",
      "Epoch: 250, TL: -0.9229529881683028, VL: -0.8332869410514832, TA: 0.9233333333333333, VA: 0.8333333333333334\n",
      "Epoch: 251, TL: -0.9372153850903528, VL: -0.7643394470214844, TA: 0.9366666666666666, VA: 0.7633333333333333\n",
      "Epoch: 252, TL: -0.941111741673503, VL: -0.8152573704719543, TA: 0.9411111111111111, VA: 0.8166666666666667\n",
      "Epoch: 253, TL: -0.9600037191880697, VL: -0.8368353247642517, TA: 0.96, VA: 0.8366666666666667\n",
      "Epoch: 254, TL: -0.9611111111871118, VL: -0.8370992541313171, TA: 0.9611111111111111, VA: 0.8366666666666667\n",
      "Epoch: 255, TL: -0.952215730824639, VL: -0.7961347103118896, TA: 0.9522222222222222, VA: 0.7966666666666666\n",
      "Epoch: 256, TL: -0.8998394585478409, VL: -0.783892810344696, TA: 0.9, VA: 0.7833333333333333\n",
      "Epoch: 257, TL: -0.9222930970649172, VL: -0.806368887424469, TA: 0.9222222222222223, VA: 0.8066666666666666\n",
      "Epoch: 258, TL: -0.9377773974256746, VL: -0.8059104084968567, TA: 0.9377777777777778, VA: 0.8066666666666666\n",
      "Epoch: 259, TL: -0.9433416754820568, VL: -0.8558241128921509, TA: 0.9433333333333334, VA: 0.8566666666666667\n",
      "Epoch: 260, TL: -0.9588888828548715, VL: -0.8559517860412598, TA: 0.9588888888888889, VA: 0.8566666666666667\n",
      "Epoch: 261, TL: -0.9577777532736549, VL: -0.8543542623519897, TA: 0.9577777777777777, VA: 0.8533333333333334\n",
      "Epoch: 262, TL: -0.9578166308296682, VL: -0.85428386926651, TA: 0.9577777777777777, VA: 0.8533333333333334\n",
      "Epoch: 263, TL: -0.9376941228624608, VL: -0.7853944301605225, TA: 0.9377777777777778, VA: 0.7866666666666666\n",
      "Epoch: 264, TL: -0.9421500586002383, VL: -0.8134960532188416, TA: 0.9422222222222222, VA: 0.8133333333333334\n",
      "Epoch: 265, TL: -0.9270661617404169, VL: -0.804283082485199, TA: 0.9277777777777778, VA: 0.8033333333333333\n",
      "Epoch: 266, TL: -0.9305121601467292, VL: -0.8443901538848877, TA: 0.93, VA: 0.8433333333333334\n",
      "Epoch: 267, TL: -0.9533313448562697, VL: -0.8480805158615112, TA: 0.9533333333333334, VA: 0.8466666666666667\n",
      "Epoch: 268, TL: -0.9520637328743751, VL: -0.8544907569885254, TA: 0.9522222222222222, VA: 0.8533333333333334\n",
      "Epoch: 269, TL: -0.9490732929239087, VL: -0.8532598614692688, TA: 0.9488888888888889, VA: 0.8533333333333334\n",
      "Epoch: 270, TL: -0.933616984190919, VL: -0.7632104754447937, TA: 0.9333333333333333, VA: 0.7633333333333333\n",
      "Epoch: 271, TL: -0.9366429682801848, VL: -0.8581653833389282, TA: 0.9366666666666666, VA: 0.8566666666666667\n",
      "Epoch: 272, TL: -0.9601395256259481, VL: -0.8647915124893188, TA: 0.96, VA: 0.8666666666666667\n",
      "Epoch: 273, TL: -0.9611110997200012, VL: -0.8623856902122498, TA: 0.9611111111111111, VA: 0.8633333333333333\n",
      "Epoch: 274, TL: -0.9600309000350535, VL: -0.8499869704246521, TA: 0.96, VA: 0.85\n",
      "Epoch: 275, TL: -0.9586298765381993, VL: -0.8533403873443604, TA: 0.9588888888888889, VA: 0.8533333333333334\n",
      "Epoch: 276, TL: -0.9510970689107014, VL: -0.7735857963562012, TA: 0.9511111111111111, VA: 0.7733333333333333\n",
      "Epoch: 277, TL: -0.9045689736037758, VL: -0.8299986124038696, TA: 0.9044444444444445, VA: 0.83\n",
      "Epoch: 278, TL: -0.9223219291756747, VL: -0.8489351868629456, TA: 0.9233333333333333, VA: 0.85\n",
      "Epoch: 279, TL: -0.9522221178478677, VL: -0.8492566347122192, TA: 0.9522222222222222, VA: 0.85\n",
      "Epoch: 280, TL: -0.9422360247525289, VL: -0.8507410287857056, TA: 0.9422222222222222, VA: 0.8533333333333334\n",
      "Epoch: 281, TL: -0.9555555555555569, VL: -0.8485554456710815, TA: 0.9555555555555556, VA: 0.85\n",
      "Epoch: 282, TL: -0.9522222216924036, VL: -0.8485790491104126, TA: 0.9522222222222222, VA: 0.85\n",
      "Epoch: 283, TL: -0.945542421194542, VL: -0.8435688614845276, TA: 0.9455555555555556, VA: 0.8433333333333334\n",
      "Epoch: 284, TL: -0.9455493503083904, VL: -0.8540731072425842, TA: 0.9455555555555556, VA: 0.8533333333333334\n",
      "Epoch: 285, TL: -0.9544439299239053, VL: -0.8437050580978394, TA: 0.9544444444444444, VA: 0.8466666666666667\n",
      "Epoch: 286, TL: -0.9599990742692414, VL: -0.8356654047966003, TA: 0.96, VA: 0.8366666666666667\n",
      "Epoch: 287, TL: -0.9534072774222391, VL: -0.8400258421897888, TA: 0.9533333333333334, VA: 0.84\n",
      "Epoch: 288, TL: -0.8933429559736704, VL: -0.772376298904419, TA: 0.8933333333333333, VA: 0.7733333333333333\n",
      "Epoch: 289, TL: -0.9190404834784224, VL: -0.7871652245521545, TA: 0.9188888888888889, VA: 0.7866666666666666\n",
      "Epoch: 290, TL: -0.9309768945790087, VL: -0.7566969394683838, TA: 0.9311111111111111, VA: 0.7566666666666667\n",
      "Epoch: 291, TL: -0.9523718565290159, VL: -0.8488380908966064, TA: 0.9522222222222222, VA: 0.85\n",
      "Epoch: 292, TL: -0.9555572983794587, VL: -0.847989559173584, TA: 0.9555555555555556, VA: 0.8466666666666667\n",
      "Epoch: 293, TL: -0.9417305314540863, VL: -0.6766672730445862, TA: 0.9422222222222222, VA: 0.6766666666666666\n",
      "Epoch: 294, TL: -0.8855666722594006, VL: -0.7799943089485168, TA: 0.8855555555555555, VA: 0.78\n",
      "Epoch: 295, TL: -0.9378657860045452, VL: -0.8366662859916687, TA: 0.9377777777777778, VA: 0.8366666666666667\n",
      "Epoch: 296, TL: -0.9568664245297975, VL: -0.8400000333786011, TA: 0.9566666666666667, VA: 0.84\n",
      "Epoch: 297, TL: -0.962177069650756, VL: -0.8333451747894287, TA: 0.9622222222222222, VA: 0.8333333333333334\n",
      "Epoch: 298, TL: -0.9588888882266151, VL: -0.8333450555801392, TA: 0.9588888888888889, VA: 0.8333333333333334\n",
      "Epoch: 299, TL: -0.9395091249888299, VL: -0.8228576183319092, TA: 0.9388888888888889, VA: 0.8233333333333334\n",
      "Epoch: 300, TL: -0.9588878566689333, VL: -0.821123480796814, TA: 0.9588888888888889, VA: 0.82\n",
      "Epoch: 301, TL: -0.9566672300950898, VL: -0.8267213702201843, TA: 0.9566666666666667, VA: 0.8266666666666667\n",
      "Epoch: 302, TL: -0.9623175197839737, VL: -0.8233291506767273, TA: 0.9622222222222222, VA: 0.8233333333333334\n",
      "Epoch: 303, TL: -0.9555555727756152, VL: -0.8299975991249084, TA: 0.9555555555555556, VA: 0.83\n",
      "Epoch: 304, TL: -0.9566666666666674, VL: -0.8299975991249084, TA: 0.9566666666666667, VA: 0.83\n",
      "Epoch: 305, TL: -0.9360207260835806, VL: -0.7966510653495789, TA: 0.9355555555555556, VA: 0.7966666666666666\n",
      "Epoch: 306, TL: -0.9032299291259824, VL: -0.8237408399581909, TA: 0.9033333333333333, VA: 0.8233333333333334\n",
      "Epoch: 307, TL: -0.936451094997682, VL: -0.8338377475738525, TA: 0.9366666666666666, VA: 0.8333333333333334\n",
      "Epoch: 308, TL: -0.9611110124566313, VL: -0.8340201377868652, TA: 0.9611111111111111, VA: 0.8333333333333334\n",
      "Epoch: 309, TL: -0.9633288317256503, VL: -0.8253716230392456, TA: 0.9633333333333334, VA: 0.8233333333333334\n",
      "Epoch: 310, TL: -0.9588898975666666, VL: -0.8333356380462646, TA: 0.9588888888888889, VA: 0.8333333333333334\n",
      "Epoch: 311, TL: -0.959999998410543, VL: -0.833324670791626, TA: 0.96, VA: 0.8333333333333334\n",
      "Epoch: 312, TL: -0.9521997666212, VL: -0.7589700222015381, TA: 0.9522222222222222, VA: 0.76\n",
      "Epoch: 313, TL: -0.9387875326270723, VL: -0.8401009440422058, TA: 0.9388888888888889, VA: 0.84\n",
      "Epoch: 314, TL: -0.9588888888888902, VL: -0.8400989770889282, TA: 0.9588888888888889, VA: 0.84\n",
      "Epoch: 315, TL: -0.9511110492547504, VL: -0.8566638231277466, TA: 0.9511111111111111, VA: 0.8566666666666667\n",
      "Epoch: 316, TL: -0.9577478944593005, VL: -0.8535991907119751, TA: 0.9577777777777777, VA: 0.8533333333333334\n",
      "Epoch: 317, TL: -0.8289357455186375, VL: -0.8169733881950378, TA: 0.8288888888888889, VA: 0.8166666666666667\n",
      "Epoch: 318, TL: -0.9246722903782653, VL: -0.7769334316253662, TA: 0.9244444444444444, VA: 0.7766666666666666\n",
      "Epoch: 319, TL: -0.9232303129727785, VL: -0.816630482673645, TA: 0.9233333333333333, VA: 0.8166666666666667\n",
      "Epoch: 320, TL: -0.9388884921621651, VL: -0.8213040828704834, TA: 0.9388888888888889, VA: 0.82\n",
      "Epoch: 321, TL: -0.9470181038884236, VL: -0.8133533000946045, TA: 0.9466666666666667, VA: 0.8133333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 322, TL: -0.8878107238123236, VL: -0.7703158259391785, TA: 0.8877777777777778, VA: 0.77\n",
      "Epoch: 323, TL: -0.9134271996255668, VL: -0.8153831362724304, TA: 0.9133333333333333, VA: 0.8166666666666667\n",
      "Epoch: 324, TL: -0.9502631583407881, VL: -0.8294571042060852, TA: 0.95, VA: 0.83\n",
      "Epoch: 325, TL: -0.9111735822161137, VL: -0.7574597597122192, TA: 0.9111111111111111, VA: 0.7566666666666667\n",
      "Epoch: 326, TL: -0.921291844518972, VL: -0.8266666531562805, TA: 0.9211111111111111, VA: 0.8266666666666667\n",
      "Epoch: 327, TL: -0.9507696522734619, VL: -0.8344448804855347, TA: 0.9511111111111111, VA: 0.8333333333333334\n",
      "Epoch: 328, TL: -0.9554500818010648, VL: -0.8399080634117126, TA: 0.9555555555555556, VA: 0.84\n",
      "Epoch: 329, TL: -0.9490059713720543, VL: -0.8333337903022766, TA: 0.95, VA: 0.8333333333333334\n",
      "Epoch: 330, TL: -0.9444442834049348, VL: -0.8368630409240723, TA: 0.9444444444444444, VA: 0.8366666666666667\n",
      "Epoch: 331, TL: -0.9490496711432962, VL: -0.8066275715827942, TA: 0.9488888888888889, VA: 0.8066666666666666\n",
      "Epoch: 332, TL: -0.9472581830811557, VL: -0.8098971843719482, TA: 0.9466666666666667, VA: 0.81\n",
      "Epoch: 333, TL: -0.952236555769871, VL: -0.8392829298973083, TA: 0.9522222222222222, VA: 0.84\n",
      "Epoch: 334, TL: -0.9268755692698889, VL: -0.8299534916877747, TA: 0.9266666666666666, VA: 0.83\n",
      "Epoch: 335, TL: -0.9309558910203894, VL: -0.7944780588150024, TA: 0.9311111111111111, VA: 0.7933333333333333\n",
      "Epoch: 336, TL: -0.9379584791377102, VL: -0.8267627954483032, TA: 0.9377777777777778, VA: 0.8266666666666667\n",
      "Epoch: 337, TL: -0.9432926634135179, VL: -0.776756227016449, TA: 0.9433333333333334, VA: 0.7766666666666666\n",
      "Epoch: 338, TL: -0.922222046163315, VL: -0.791313111782074, TA: 0.9222222222222223, VA: 0.79\n",
      "Epoch: 339, TL: -0.9345035321248188, VL: -0.8100497126579285, TA: 0.9344444444444444, VA: 0.81\n",
      "Epoch: 340, TL: -0.9444443517727045, VL: -0.8231973052024841, TA: 0.9444444444444444, VA: 0.8233333333333334\n",
      "Epoch: 341, TL: -0.9355530002012973, VL: -0.8033177256584167, TA: 0.9355555555555556, VA: 0.8033333333333333\n",
      "Epoch: 342, TL: -0.9233333179298564, VL: -0.8032322525978088, TA: 0.9233333333333333, VA: 0.8033333333333333\n",
      "Epoch: 343, TL: -0.9433289099685596, VL: -0.8365193009376526, TA: 0.9433333333333334, VA: 0.8366666666666667\n",
      "Epoch: 344, TL: -0.9440132440039435, VL: -0.7534920573234558, TA: 0.9444444444444444, VA: 0.7533333333333333\n",
      "Epoch: 345, TL: -0.9155937742979041, VL: -0.7796318531036377, TA: 0.9155555555555556, VA: 0.78\n",
      "Epoch: 346, TL: -0.9317601700064347, VL: -0.8332995772361755, TA: 0.9322222222222222, VA: 0.8333333333333334\n",
      "Epoch: 347, TL: -0.9410815831207825, VL: -0.8413013219833374, TA: 0.9411111111111111, VA: 0.84\n",
      "Epoch: 348, TL: -0.9469031579504865, VL: -0.8199870586395264, TA: 0.9466666666666667, VA: 0.82\n",
      "Epoch: 349, TL: -0.9455797554143209, VL: -0.7576508522033691, TA: 0.9455555555555556, VA: 0.7566666666666667\n",
      "Epoch: 350, TL: -0.8936878368836086, VL: -0.7605913877487183, TA: 0.8933333333333333, VA: 0.76\n",
      "Epoch: 351, TL: -0.9044802965945984, VL: -0.7933410406112671, TA: 0.9044444444444445, VA: 0.7933333333333333\n",
      "Epoch: 352, TL: -0.8687308321143471, VL: -0.7766671180725098, TA: 0.8688888888888889, VA: 0.7766666666666666\n",
      "Epoch: 353, TL: -0.9448170975642219, VL: -0.8466651439666748, TA: 0.9444444444444444, VA: 0.8466666666666667\n",
      "Epoch: 354, TL: -0.9059569029872316, VL: -0.6917070150375366, TA: 0.9066666666666666, VA: 0.6933333333333334\n",
      "Epoch: 355, TL: -0.8740866936720942, VL: -0.838021993637085, TA: 0.8744444444444445, VA: 0.8366666666666667\n",
      "Epoch: 356, TL: -0.9467391348216289, VL: -0.8105447888374329, TA: 0.9466666666666667, VA: 0.81\n",
      "Epoch: 357, TL: -0.941090857170349, VL: -0.8433325290679932, TA: 0.9411111111111111, VA: 0.8433333333333334\n",
      "Epoch: 358, TL: -0.9543501425471133, VL: -0.849895715713501, TA: 0.9544444444444444, VA: 0.85\n",
      "Epoch: 359, TL: -0.9467171661941584, VL: -0.8464704751968384, TA: 0.9466666666666667, VA: 0.8466666666666667\n",
      "Epoch: 360, TL: -0.9488897616129952, VL: -0.8405612707138062, TA: 0.9488888888888889, VA: 0.84\n",
      "Epoch: 361, TL: -0.8904134180169948, VL: -0.8466573357582092, TA: 0.8911111111111111, VA: 0.8466666666666667\n",
      "Epoch: 362, TL: -0.9407110023877678, VL: -0.820919930934906, TA: 0.9411111111111111, VA: 0.82\n",
      "Epoch: 363, TL: -0.9455555992379838, VL: -0.8175622224807739, TA: 0.9455555555555556, VA: 0.8166666666666667\n",
      "Epoch: 364, TL: -0.9404548472541323, VL: -0.8526570200920105, TA: 0.94, VA: 0.8533333333333334\n",
      "Epoch: 365, TL: -0.9522235950318974, VL: -0.8533376455307007, TA: 0.9522222222222222, VA: 0.8533333333333334\n",
      "Epoch: 366, TL: -0.9533226480611517, VL: -0.8598582148551941, TA: 0.9533333333333334, VA: 0.86\n",
      "Epoch: 367, TL: -0.9394642459413853, VL: -0.8299936652183533, TA: 0.94, VA: 0.83\n",
      "Epoch: 368, TL: -0.9466279124587837, VL: -0.8333361148834229, TA: 0.9466666666666667, VA: 0.8333333333333334\n",
      "Epoch: 369, TL: -0.9477772080236011, VL: -0.8368350863456726, TA: 0.9477777777777778, VA: 0.8366666666666667\n",
      "Epoch: 370, TL: -0.9450636154973105, VL: -0.7935441136360168, TA: 0.9455555555555556, VA: 0.7933333333333333\n",
      "Epoch: 371, TL: -0.9344041300341344, VL: -0.8566721081733704, TA: 0.9344444444444444, VA: 0.8566666666666667\n",
      "Epoch: 372, TL: -0.9566636606720568, VL: -0.856667697429657, TA: 0.9566666666666667, VA: 0.8566666666666667\n",
      "Epoch: 373, TL: -0.879093804464498, VL: -0.7732779383659363, TA: 0.8788888888888889, VA: 0.7733333333333333\n",
      "Epoch: 374, TL: -0.9477366853206601, VL: -0.8395077586174011, TA: 0.9477777777777778, VA: 0.84\n",
      "Epoch: 375, TL: -0.9544445815784979, VL: -0.8429335951805115, TA: 0.9544444444444444, VA: 0.8433333333333334\n",
      "Epoch: 376, TL: -0.9588888441191813, VL: -0.8399314284324646, TA: 0.9588888888888889, VA: 0.84\n",
      "Epoch: 377, TL: -0.9477675219020456, VL: -0.8071052432060242, TA: 0.9477777777777778, VA: 0.8066666666666666\n",
      "Epoch: 378, TL: -0.9355713617854209, VL: -0.8373479843139648, TA: 0.9355555555555556, VA: 0.8366666666666667\n",
      "Epoch: 379, TL: -0.9433346520530427, VL: -0.8116706013679504, TA: 0.9433333333333334, VA: 0.8133333333333334\n",
      "Epoch: 380, TL: -0.9422222154645219, VL: -0.8081895709037781, TA: 0.9422222222222222, VA: 0.8066666666666666\n",
      "Epoch: 381, TL: -0.9323011878824748, VL: -0.836017906665802, TA: 0.9322222222222222, VA: 0.8366666666666667\n",
      "Epoch: 382, TL: -0.9583806709511018, VL: -0.8433501720428467, TA: 0.9588888888888889, VA: 0.8433333333333334\n",
      "Epoch: 383, TL: -0.9198414810317482, VL: -0.76523756980896, TA: 0.92, VA: 0.7633333333333333\n",
      "Epoch: 384, TL: -0.9386463606783216, VL: -0.8096635341644287, TA: 0.9388888888888889, VA: 0.81\n",
      "Epoch: 385, TL: -0.9366430304714195, VL: -0.8099685311317444, TA: 0.9366666666666666, VA: 0.81\n",
      "Epoch: 386, TL: -0.9301872831809166, VL: -0.7828754782676697, TA: 0.93, VA: 0.7833333333333333\n",
      "Epoch: 387, TL: -0.9466740371102318, VL: -0.8130570650100708, TA: 0.9466666666666667, VA: 0.8133333333333334\n",
      "Epoch: 388, TL: -0.9433333814717938, VL: -0.8132376670837402, TA: 0.9433333333333334, VA: 0.8133333333333334\n",
      "Epoch: 389, TL: -0.9544454085316205, VL: -0.8348448872566223, TA: 0.9544444444444444, VA: 0.8333333333333334\n",
      "Epoch: 390, TL: -0.9600000346070442, VL: -0.8399741053581238, TA: 0.96, VA: 0.84\n",
      "Epoch: 391, TL: -0.9655548723538717, VL: -0.839941680431366, TA: 0.9655555555555555, VA: 0.84\n",
      "Epoch: 392, TL: -0.9644444444445882, VL: -0.8399416208267212, TA: 0.9644444444444444, VA: 0.84\n",
      "Epoch: 393, TL: -0.9622222222222222, VL: -0.8399416208267212, TA: 0.9622222222222222, VA: 0.84\n",
      "Epoch: 394, TL: -0.9298934336479442, VL: -0.7566670179367065, TA: 0.93, VA: 0.7566666666666667\n",
      "Epoch: 395, TL: -0.9478843406708054, VL: -0.8353815674781799, TA: 0.9477777777777778, VA: 0.8366666666666667\n",
      "Epoch: 396, TL: -0.9544444435172611, VL: -0.8353059887886047, TA: 0.9544444444444444, VA: 0.8366666666666667\n",
      "Epoch: 397, TL: -0.957777783113242, VL: -0.838914692401886, TA: 0.9577777777777777, VA: 0.8366666666666667\n",
      "Epoch: 398, TL: -0.957781431985553, VL: -0.8355926871299744, TA: 0.9577777777777777, VA: 0.8366666666666667\n",
      "Epoch: 399, TL: -0.9577744618237005, VL: -0.84333336353302, TA: 0.9577777777777777, VA: 0.8433333333333334\n",
      "Epoch: 400, TL: -0.9500199691868491, VL: -0.7600001096725464, TA: 0.95, VA: 0.76\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0cb1cff2a792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m22050\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelConv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_out_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-afe209593b30>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(model, loader, validation_loader, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mpredictions_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projekte/deeplearning/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projekte/deeplearning/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projekte/deeplearning/venv/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e0686e893cec>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mSONG_LENGTH_SECONDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projekte/deeplearning/venv/lib/python3.7/site-packages/scipy/signal/signaltools.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(x, num, t, axis, window)\u001b[0m\n\u001b[1;32m   2225\u001b[0m     \"\"\"\n\u001b[1;32m   2226\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2227\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfftpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2228\u001b[0m     \u001b[0mNx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projekte/deeplearning/venv/lib/python3.7/site-packages/scipy/fftpack/basic.py\u001b[0m in \u001b[0;36mfft\u001b[0;34m(x, n, axis, overwrite_x)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwork_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moverwrite_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_size, loader, validation_loader = load_dataset(add_dimension=True, downsample=22050, noise=True)\n",
    "model = ModelConv2(input_size, kernel_size=5, conv_out_channels=10, linear_size=500).cuda()\n",
    "learn(model, loader, validation_loader, epochs=10000, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
