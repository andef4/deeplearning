4 linear layers with 500 neurons each, Dropout after every layer.
Epoch: 0, Train Loss: -0.33289360006650287, Validation Loss: -0.3331930935382843
Epoch: 1, Train Loss: -0.3334940089119805, Validation Loss: -0.33196818828582764
Epoch: 2, Train Loss: -0.3333748075697157, Validation Loss: -0.33096033334732056
Epoch: 3, Train Loss: -0.33371353215641447, Validation Loss: -0.3297278881072998
Epoch: 4, Train Loss: -0.3349907590283288, Validation Loss: -0.3278571367263794
Epoch: 5, Train Loss: -0.3346696502632565, Validation Loss: -0.32603275775909424
Epoch: 6, Train Loss: -0.33359672294722664, Validation Loss: -0.3247658610343933
Epoch: 7, Train Loss: -0.3335256901052263, Validation Loss: -0.3236547112464905
Epoch: 8, Train Loss: -0.33752112090587616, Validation Loss: -0.32125169038772583
Epoch: 9, Train Loss: -0.33653772738244797, Validation Loss: -0.3190702795982361
Epoch: 10, Train Loss: -0.3395332147677739, Validation Loss: -0.3166424632072449
Epoch: 11, Train Loss: -0.3414213730229272, Validation Loss: -0.3148978054523468
Epoch: 12, Train Loss: -0.3399674614270528, Validation Loss: -0.31353387236595154
Epoch: 13, Train Loss: -0.34153862131966484, Validation Loss: -0.31285691261291504
Epoch: 14, Train Loss: -0.34064033528168997, Validation Loss: -0.3122605085372925
Epoch: 15, Train Loss: -0.34379169907834795, Validation Loss: -0.312735378742218
Epoch: 16, Train Loss: -0.340457965599166, Validation Loss: -0.3123615086078644
Epoch: 17, Train Loss: -0.34070714281664954, Validation Loss: -0.3120840787887573
Epoch: 18, Train Loss: -0.3430752416451772, Validation Loss: -0.3126772344112396
Epoch: 19, Train Loss: -0.34561256302727594, Validation Loss: -0.31388962268829346
Epoch: 20, Train Loss: -0.3434803671307034, Validation Loss: -0.31442129611968994
Epoch: 21, Train Loss: -0.3583397517601649, Validation Loss: -0.3169402778148651
Epoch: 22, Train Loss: -0.3740779442919625, Validation Loss: -0.321271151304245
Epoch: 23, Train Loss: -0.44205343127250674, Validation Loss: -0.32654640078544617
Epoch: 24, Train Loss: -0.5160594311025407, Validation Loss: -0.329202800989151
Epoch: 25, Train Loss: -0.5580250892374251, Validation Loss: -0.3359871208667755
Epoch: 26, Train Loss: -0.5752411597304874, Validation Loss: -0.33472150564193726
Epoch: 27, Train Loss: -0.5997524552875095, Validation Loss: -0.32296469807624817
Epoch: 28, Train Loss: -0.6159556892183092, Validation Loss: -0.32638871669769287
Epoch: 29, Train Loss: -0.6056304633617401, Validation Loss: -0.32432129979133606
Epoch: 30, Train Loss: -0.6162906812296973, Validation Loss: -0.31204065680503845
Epoch: 31, Train Loss: -0.6259942836231656, Validation Loss: -0.3213847577571869
Epoch: 32, Train Loss: -0.6325086275736491, Validation Loss: -0.3335137665271759
Epoch: 33, Train Loss: -0.6454436825381384, Validation Loss: -0.33277398347854614
Epoch: 34, Train Loss: -0.6583124902513292, Validation Loss: -0.3455650210380554
Epoch: 35, Train Loss: -0.6833103789223565, Validation Loss: -0.359149694442749
Epoch: 36, Train Loss: -0.6968586749500698, Validation Loss: -0.35842660069465637
Epoch: 37, Train Loss: -0.6975493570168813, Validation Loss: -0.3812423348426819
Epoch: 38, Train Loss: -0.7046302265591091, Validation Loss: -0.38908979296684265
Epoch: 39, Train Loss: -0.7290285322401259, Validation Loss: -0.33125200867652893
Epoch: 40, Train Loss: -0.7368270556131998, Validation Loss: -0.366312175989151
Epoch: 41, Train Loss: -0.7548203653759427, Validation Loss: -0.36158090829849243
Epoch: 42, Train Loss: -0.7502660446696812, Validation Loss: -0.38184866309165955
Epoch: 43, Train Loss: -0.7752380861176384, Validation Loss: -0.37282422184944153
Epoch: 44, Train Loss: -0.795771492852105, Validation Loss: -0.36026614904403687
Epoch: 45, Train Loss: -0.8003880156411065, Validation Loss: -0.3789459764957428
Epoch: 46, Train Loss: -0.7639103478855557, Validation Loss: -0.37264326214790344
Epoch: 47, Train Loss: -0.7895836022165087, Validation Loss: -0.38323765993118286
Epoch: 48, Train Loss: -0.7910025013817681, Validation Loss: -0.4032318890094757
Epoch: 49, Train Loss: -0.7419846064514584, Validation Loss: -0.3665107786655426
Epoch: 50, Train Loss: -0.7391170369254219, Validation Loss: -0.35998207330703735
Epoch: 51, Train Loss: -0.7246585011482238, Validation Loss: -0.32952022552490234
Epoch: 52, Train Loss: -0.8171347988976373, Validation Loss: -0.3589719235897064
Epoch: 53, Train Loss: -0.8095663732952542, Validation Loss: -0.35192805528640747
Epoch: 54, Train Loss: -0.8215456075138516, Validation Loss: -0.33948224782943726
Epoch: 55, Train Loss: -0.8228465888235305, Validation Loss: -0.3480300307273865
Epoch: 56, Train Loss: -0.8163837684525383, Validation Loss: -0.3807910978794098
Epoch: 57, Train Loss: -0.8073904010984633, Validation Loss: -0.34029000997543335
Epoch: 58, Train Loss: -0.8267133368386163, Validation Loss: -0.3727555274963379
Epoch: 59, Train Loss: -0.8363630943828159, Validation Loss: -0.37633180618286133
Epoch: 60, Train Loss: -0.8462615927060445, Validation Loss: -0.3546329140663147
Epoch: 61, Train Loss: -0.8352207528220282, Validation Loss: -0.34861230850219727
Epoch: 62, Train Loss: -0.8365139947997199, Validation Loss: -0.3573976457118988
Epoch: 63, Train Loss: -0.8395620028177897, Validation Loss: -0.363447368144989
Epoch: 64, Train Loss: -0.8429828604062398, Validation Loss: -0.3687605559825897
Epoch: 65, Train Loss: -0.8490314536624485, Validation Loss: -0.36107099056243896
Epoch: 66, Train Loss: -0.8514793846342299, Validation Loss: -0.34070485830307007
Epoch: 67, Train Loss: -0.8458287941084968, Validation Loss: -0.38236770033836365
Epoch: 68, Train Loss: -0.8600175658861796, Validation Loss: -0.3557584881782532
Epoch: 69, Train Loss: -0.8574441591898601, Validation Loss: -0.37625864148139954
Epoch: 70, Train Loss: -0.8580754770172967, Validation Loss: -0.35382264852523804
Epoch: 71, Train Loss: -0.8681851108868917, Validation Loss: -0.3691226541996002
Epoch: 72, Train Loss: -0.8576815406481425, Validation Loss: -0.39046305418014526
Epoch: 73, Train Loss: -0.8469136357307434, Validation Loss: -0.38124120235443115
Epoch: 74, Train Loss: -0.8574319375885857, Validation Loss: -0.35366079211235046
Epoch: 75, Train Loss: -0.8646208948559231, Validation Loss: -0.3776446580886841
Epoch: 76, Train Loss: -0.8478367355134752, Validation Loss: -0.38329392671585083
Epoch: 77, Train Loss: -0.8510510789023505, Validation Loss: -0.3518007695674896
Epoch: 78, Train Loss: -0.8719898594750298, Validation Loss: -0.3521384596824646
Epoch: 79, Train Loss: -0.868171481291453, Validation Loss: -0.35448139905929565
Epoch: 80, Train Loss: -0.8747584753566318, Validation Loss: -0.37477797269821167
Epoch: 81, Train Loss: -0.8841087063153584, Validation Loss: -0.3531166911125183
Epoch: 82, Train Loss: -0.886767221821679, Validation Loss: -0.3637852370738983
Epoch: 83, Train Loss: -0.8704978677961561, Validation Loss: -0.340025931596756
Epoch: 84, Train Loss: -0.8709113438924153, Validation Loss: -0.38236451148986816
Epoch: 85, Train Loss: -0.8776693900426229, Validation Loss: -0.35809126496315
Epoch: 86, Train Loss: -0.880034269226922, Validation Loss: -0.3649483919143677
Epoch: 87, Train Loss: -0.8855034841431512, Validation Loss: -0.367634654045105
Epoch: 88, Train Loss: -0.8747130168808831, Validation Loss: -0.35275208950042725
Epoch: 89, Train Loss: -0.8764692385991414, Validation Loss: -0.3587033748626709
Epoch: 90, Train Loss: -0.8715101970566643, Validation Loss: -0.3525829315185547
Epoch: 91, Train Loss: -0.8778766777780321, Validation Loss: -0.35467109084129333
Epoch: 92, Train Loss: -0.8768541693687439, Validation Loss: -0.35796743631362915
Epoch: 93, Train Loss: -0.8724851515558031, Validation Loss: -0.35665416717529297
Epoch: 94, Train Loss: -0.8883530325359769, Validation Loss: -0.3576587438583374
Epoch: 95, Train Loss: -0.8626390430662367, Validation Loss: -0.3606666624546051
Epoch: 96, Train Loss: -0.8785013066397773, Validation Loss: -0.3633972108364105
Epoch: 97, Train Loss: -0.8969445400767856, Validation Loss: -0.3634093105792999
Epoch: 98, Train Loss: -0.875714107354482, Validation Loss: -0.36501339077949524
Epoch: 99, Train Loss: -0.8735106733110216, Validation Loss: -0.3618834316730499

9 layers, 500 neurons each, dropout after each layer, learning rate 0.0001
model = Model1Linear(500).cuda()

learn(model, 10000000)

Epoch: 0, Train Loss: -0.33314572241571216, Validation Loss: -0.33336856961250305
Epoch: 1, Train Loss: -0.3331556585099962, Validation Loss: -0.33331653475761414
Epoch: 2, Train Loss: -0.33349309696091545, Validation Loss: -0.33325740694999695
Epoch: 3, Train Loss: -0.33336908883518646, Validation Loss: -0.3331812620162964
Epoch: 4, Train Loss: -0.33321265975634257, Validation Loss: -0.3330972492694855
Epoch: 5, Train Loss: -0.3331224044164022, Validation Loss: -0.3330419361591339
Epoch: 6, Train Loss: -0.3333868649270799, Validation Loss: -0.33295851945877075
Epoch: 7, Train Loss: -0.3333988395002153, Validation Loss: -0.33291593194007874
Epoch: 8, Train Loss: -0.3336293147669898, Validation Loss: -0.3328177034854889
Epoch: 9, Train Loss: -0.33350832131173874, Validation Loss: -0.3326793313026428
Epoch: 10, Train Loss: -0.3338126619656881, Validation Loss: -0.332522988319397
Epoch: 11, Train Loss: -0.3331885947121514, Validation Loss: -0.3324414789676666
Epoch: 12, Train Loss: -0.333680702580346, Validation Loss: -0.3322843909263611
Epoch: 13, Train Loss: -0.3336702161365085, Validation Loss: -0.332194983959198
Epoch: 14, Train Loss: -0.3334379129939609, Validation Loss: -0.3320271670818329
Epoch: 15, Train Loss: -0.33382053507698906, Validation Loss: -0.3318488597869873
Epoch: 16, Train Loss: -0.3334131479263306, Validation Loss: -0.3316279351711273
Epoch: 17, Train Loss: -0.33332479265001086, Validation Loss: -0.3315327763557434
Epoch: 18, Train Loss: -0.3341212173302968, Validation Loss: -0.3312712013721466
Epoch: 19, Train Loss: -0.33429909149805703, Validation Loss: -0.3309006690979004
Epoch: 20, Train Loss: -0.33443394833140905, Validation Loss: -0.3302190899848938
Epoch: 21, Train Loss: -0.3337990118397607, Validation Loss: -0.329673707485199
Epoch: 22, Train Loss: -0.33468739853964913, Validation Loss: -0.32913464307785034
Epoch: 23, Train Loss: -0.33410650425487093, Validation Loss: -0.32864493131637573
Epoch: 24, Train Loss: -0.33378288216061064, Validation Loss: -0.32802116870880127
Epoch: 25, Train Loss: -0.33548286457856497, Validation Loss: -0.32700932025909424
Epoch: 26, Train Loss: -0.33424913419617547, Validation Loss: -0.32647427916526794
Epoch: 27, Train Loss: -0.3340734955337312, Validation Loss: -0.3256036341190338
Epoch: 28, Train Loss: -0.33673664728800456, Validation Loss: -0.3249712586402893
Epoch: 29, Train Loss: -0.3368980904420217, Validation Loss: -0.3242371678352356
Epoch: 30, Train Loss: -0.33641942342122394, Validation Loss: -0.32375568151474
Epoch: 31, Train Loss: -0.3384998927513758, Validation Loss: -0.32299140095710754
Epoch: 32, Train Loss: -0.33533795972665154, Validation Loss: -0.3227202594280243
Epoch: 33, Train Loss: -0.33801534838146635, Validation Loss: -0.3220110535621643
Epoch: 34, Train Loss: -0.34055766065915427, Validation Loss: -0.321739137172699
Epoch: 35, Train Loss: -0.3372937884595659, Validation Loss: -0.3216348886489868
Epoch: 36, Train Loss: -0.33929480529493755, Validation Loss: -0.3212902843952179
Epoch: 37, Train Loss: -0.33861074017153847, Validation Loss: -0.321174293756485
Epoch: 38, Train Loss: -0.33684203988975947, Validation Loss: -0.32118210196495056
Epoch: 39, Train Loss: -0.3373935689528783, Validation Loss: -0.32068249583244324
Epoch: 40, Train Loss: -0.33969310820102694, Validation Loss: -0.3207186460494995
Epoch: 41, Train Loss: -0.33631847235891554, Validation Loss: -0.32071393728256226
Epoch: 42, Train Loss: -0.3391679826709959, Validation Loss: -0.3207346796989441
Epoch: 43, Train Loss: -0.3372430952058898, Validation Loss: -0.3206655979156494
Epoch: 44, Train Loss: -0.3389376911852095, Validation Loss: -0.32065892219543457
Epoch: 45, Train Loss: -0.33701322144932216, Validation Loss: -0.320516437292099
Epoch: 46, Train Loss: -0.33631965816020964, Validation Loss: -0.32032182812690735
Epoch: 47, Train Loss: -0.33652984698613486, Validation Loss: -0.3202269375324249
Epoch: 48, Train Loss: -0.33795608696010376, Validation Loss: -0.32024580240249634
Epoch: 49, Train Loss: -0.3375000069538752, Validation Loss: -0.32024264335632324
Epoch: 50, Train Loss: -0.3375842697090573, Validation Loss: -0.32018911838531494
Epoch: 51, Train Loss: -0.33727665063407686, Validation Loss: -0.3201729655265808
Epoch: 52, Train Loss: -0.33694030261702007, Validation Loss: -0.32010576128959656
Epoch: 53, Train Loss: -0.33914916813373563, Validation Loss: -0.3202102780342102
Epoch: 54, Train Loss: -0.34016989121834434, Validation Loss: -0.32047998905181885
Epoch: 55, Train Loss: -0.33927190171347726, Validation Loss: -0.32047516107559204
Epoch: 56, Train Loss: -0.3388702869415283, Validation Loss: -0.3206592798233032
Epoch: 57, Train Loss: -0.33838870657814873, Validation Loss: -0.3204207718372345
Epoch: 58, Train Loss: -0.33893683320946166, Validation Loss: -0.3204902410507202
Epoch: 59, Train Loss: -0.3373389883173837, Validation Loss: -0.3206180930137634
Epoch: 60, Train Loss: -0.34024506045712366, Validation Loss: -0.320974200963974
Epoch: 61, Train Loss: -0.34117627176973553, Validation Loss: -0.32132187485694885
Epoch: 62, Train Loss: -0.3410545825958252, Validation Loss: -0.32151368260383606
Epoch: 63, Train Loss: -0.347392111354404, Validation Loss: -0.3222334384918213
Epoch: 64, Train Loss: -0.34775568577978344, Validation Loss: -0.3230201005935669
Epoch: 65, Train Loss: -0.3562529209587309, Validation Loss: -0.3237755298614502
Epoch: 66, Train Loss: -0.3686025063196818, Validation Loss: -0.328145295381546
Epoch: 67, Train Loss: -0.42885182764795093, Validation Loss: -0.33052510023117065
Epoch: 68, Train Loss: -0.512804651260376, Validation Loss: -0.32848113775253296
Epoch: 69, Train Loss: -0.5859743257363638, Validation Loss: -0.3634549379348755
Epoch: 70, Train Loss: -0.614505840672387, Validation Loss: -0.4320160448551178
Epoch: 71, Train Loss: -0.6217271639241113, Validation Loss: -0.30398622155189514
Epoch: 72, Train Loss: -0.6789108130666944, Validation Loss: -0.3121694326400757
Epoch: 73, Train Loss: -0.69897164967325, Validation Loss: -0.47527241706848145
Epoch: 74, Train Loss: -0.732821344004737, Validation Loss: -0.44616520404815674
Epoch: 75, Train Loss: -0.7625628431638082, Validation Loss: -0.49931690096855164
Epoch: 76, Train Loss: -0.7509831156995561, Validation Loss: -0.42316171526908875
Epoch: 77, Train Loss: -0.7369523829883999, Validation Loss: -0.2949454188346863
Epoch: 78, Train Loss: -0.7743158433172438, Validation Loss: -0.4799875020980835
Epoch: 79, Train Loss: -0.7940122789806789, Validation Loss: -0.41867560148239136
Epoch: 80, Train Loss: -0.8075049559275309, Validation Loss: -0.4558582305908203
Epoch: 81, Train Loss: -0.7976109756363763, Validation Loss: -0.38491976261138916
Epoch: 82, Train Loss: -0.8164221551683214, Validation Loss: -0.46380099654197693
Epoch: 83, Train Loss: -0.8275533239046733, Validation Loss: -0.45991700887680054
Epoch: 84, Train Loss: -0.8205538087420994, Validation Loss: -0.5008504986763
Epoch: 85, Train Loss: -0.8405568692419264, Validation Loss: -0.48666930198669434
Epoch: 86, Train Loss: -0.8558174093564351, Validation Loss: -0.4902768135070801
Epoch: 87, Train Loss: -0.8768745130962796, Validation Loss: -0.46550726890563965
Epoch: 88, Train Loss: -0.8743411302566528, Validation Loss: -0.46225497126579285
Epoch: 89, Train Loss: -0.8773166841930813, Validation Loss: -0.4709993600845337
Epoch: 90, Train Loss: -0.8599981440438165, Validation Loss: -0.4559226334095001
Epoch: 91, Train Loss: -0.8577144331402249, Validation Loss: -0.4782986342906952
Epoch: 92, Train Loss: -0.8888220906257629, Validation Loss: -0.4683004915714264
Epoch: 93, Train Loss: -0.883671698305342, Validation Loss: -0.49442750215530396
Epoch: 94, Train Loss: -0.8974033660358853, Validation Loss: -0.47740811109542847
Epoch: 95, Train Loss: -0.9023229930135939, Validation Loss: -0.4612138271331787
Epoch: 96, Train Loss: -0.8714442703458998, Validation Loss: -0.4802122414112091
Epoch: 97, Train Loss: -0.8654533770349291, Validation Loss: -0.4528602957725525
Epoch: 98, Train Loss: -0.9083573301633199, Validation Loss: -0.48271963000297546
Epoch: 99, Train Loss: -0.9073376139005025, Validation Loss: -0.470594197511673
Epoch: 100, Train Loss: -0.8771861804856195, Validation Loss: -0.4664021134376526
Epoch: 101, Train Loss: -0.8913892043961419, Validation Loss: -0.4688376784324646
Epoch: 102, Train Loss: -0.9130618466271294, Validation Loss: -0.48016229271888733
Epoch: 103, Train Loss: -0.8851890219582452, Validation Loss: -0.4839172661304474
Epoch: 104, Train Loss: -0.9175195508533054, Validation Loss: -0.46887072920799255
Epoch: 105, Train Loss: -0.8963893334070842, Validation Loss: -0.4730548560619354
Epoch: 106, Train Loss: -0.9058235128720601, Validation Loss: -0.4695022404193878
Epoch: 107, Train Loss: -0.9144522070884704, Validation Loss: -0.5116673111915588
Epoch: 108, Train Loss: -0.9234087506930033, Validation Loss: -0.4939410984516144
Epoch: 109, Train Loss: -0.9104199621412489, Validation Loss: -0.4648241102695465
Epoch: 110, Train Loss: -0.9172139975759718, Validation Loss: -0.4848664402961731
Epoch: 111, Train Loss: -0.9258627732594807, Validation Loss: -0.47382259368896484
Epoch: 112, Train Loss: -0.9202168173260159, Validation Loss: -0.4874117970466614
Epoch: 113, Train Loss: -0.9227433376842075, Validation Loss: -0.4672037363052368
Epoch: 114, Train Loss: -0.9312235487831964, Validation Loss: -0.520950198173523
Epoch: 115, Train Loss: -0.9278184625837538, Validation Loss: -0.5099270939826965
Epoch: 116, Train Loss: -0.9001494473881192, Validation Loss: -0.4693884253501892
Epoch: 117, Train Loss: -0.9268474486139086, Validation Loss: -0.48496100306510925
Epoch: 118, Train Loss: -0.9381949663162231, Validation Loss: -0.49451014399528503
Epoch: 119, Train Loss: -0.8866103437211779, Validation Loss: -0.4967789351940155
Epoch: 120, Train Loss: -0.907652903927697, Validation Loss: -0.4998474717140198
Epoch: 121, Train Loss: -0.9288541422949896, Validation Loss: -0.4757159650325775
Epoch: 122, Train Loss: -0.9244704458448622, Validation Loss: -0.4846765995025635
Epoch: 123, Train Loss: -0.9314074688487582, Validation Loss: -0.4752333164215088
Epoch: 124, Train Loss: -0.9338656955295139, Validation Loss: -0.5006210207939148
Epoch: 125, Train Loss: -0.9494845655229357, Validation Loss: -0.49574175477027893
Epoch: 126, Train Loss: -0.9394787006907993, Validation Loss: -0.48932528495788574
Epoch: 127, Train Loss: -0.9226499160130819, Validation Loss: -0.5065502524375916
Epoch: 128, Train Loss: -0.937999180952708, Validation Loss: -0.49524879455566406
Epoch: 129, Train Loss: -0.9358086334334479, Validation Loss: -0.47498250007629395
Epoch: 130, Train Loss: -0.9486721621619331, Validation Loss: -0.4936690330505371
Epoch: 131, Train Loss: -0.9516173177295261, Validation Loss: -0.4748060405254364
Epoch: 132, Train Loss: -0.9241784281200833, Validation Loss: -0.4727206528186798
Epoch: 133, Train Loss: -0.9345960723029243, Validation Loss: -0.4996076226234436
Epoch: 134, Train Loss: -0.9416612876786126, Validation Loss: -0.4694722294807434
Epoch: 135, Train Loss: -0.9455713033676147, Validation Loss: -0.46137920022010803
Epoch: 136, Train Loss: -0.9538344343503317, Validation Loss: -0.469101220369339
Epoch: 137, Train Loss: -0.9515028052859836, Validation Loss: -0.4927932024002075
Epoch: 138, Train Loss: -0.9395647260877821, Validation Loss: -0.470523476600647
Epoch: 139, Train Loss: -0.9474156869782342, Validation Loss: -0.470270574092865
Epoch: 140, Train Loss: -0.952695349852244, Validation Loss: -0.4860013723373413
Epoch: 141, Train Loss: -0.9424876981311374, Validation Loss: -0.47976458072662354
Epoch: 142, Train Loss: -0.9365836527612474, Validation Loss: -0.501502275466919
Epoch: 143, Train Loss: -0.9541027704874675, Validation Loss: -0.47572094202041626
Epoch: 144, Train Loss: -0.9536409510506524, Validation Loss: -0.493465393781662
Epoch: 145, Train Loss: -0.9583963553110758, Validation Loss: -0.48981398344039917
Epoch: 146, Train Loss: -0.9656725102000766, Validation Loss: -0.479930579662323
Epoch: 147, Train Loss: -0.9485415048069424, Validation Loss: -0.488826185464859
Epoch: 148, Train Loss: -0.9409356925222608, Validation Loss: -0.4798758327960968
Epoch: 149, Train Loss: -0.92957481013404, Validation Loss: -0.47177618741989136
Epoch: 150, Train Loss: -0.9654788149727715, Validation Loss: -0.4861813485622406
Epoch: 151, Train Loss: -0.9467428869671292, Validation Loss: -0.46853357553482056
Epoch: 152, Train Loss: -0.9645334614647759, Validation Loss: -0.481478214263916
Epoch: 153, Train Loss: -0.9541534927156237, Validation Loss: -0.4813506603240967
Epoch: 154, Train Loss: -0.9539658122592503, Validation Loss: -0.4909987151622772
Epoch: 155, Train Loss: -0.9494138757387797, Validation Loss: -0.47372961044311523
Epoch: 156, Train Loss: -0.9452013797230191, Validation Loss: -0.4792728126049042
Epoch: 157, Train Loss: -0.9521558536423578, Validation Loss: -0.48259609937667847
Epoch: 158, Train Loss: -0.9393148408995734, Validation Loss: -0.49326565861701965
Epoch: 159, Train Loss: -0.9543046024110582, Validation Loss: -0.4963326156139374
Epoch: 160, Train Loss: -0.9607258478800456, Validation Loss: -0.48298850655555725
Epoch: 161, Train Loss: -0.9687689781188965, Validation Loss: -0.46957114338874817
Epoch: 162, Train Loss: -0.9458525604671902, Validation Loss: -0.4801235795021057
Epoch: 163, Train Loss: -0.9497215006086561, Validation Loss: -0.5129626393318176
Epoch: 164, Train Loss: -0.9465889652570089, Validation Loss: -0.47896602749824524
Epoch: 165, Train Loss: -0.9669587320751614, Validation Loss: -0.4760753810405731
Epoch: 166, Train Loss: -0.958935617076026, Validation Loss: -0.4725949764251709
Epoch: 167, Train Loss: -0.9600605699751112, Validation Loss: -0.47713637351989746
Epoch: 168, Train Loss: -0.948942138089074, Validation Loss: -0.45917561650276184
Epoch: 169, Train Loss: -0.9463137997521295, Validation Loss: -0.4796784520149231
Epoch: 170, Train Loss: -0.9664531840218438, Validation Loss: -0.47134459018707275
Epoch: 171, Train Loss: -0.9418835388289557, Validation Loss: -0.4752957224845886
Epoch: 172, Train Loss: -0.9550255921151903, Validation Loss: -0.47934943437576294
Epoch: 173, Train Loss: -0.9632080369525485, Validation Loss: -0.5130223631858826
Epoch: 174, Train Loss: -0.9425059583452012, Validation Loss: -0.49533897638320923
Epoch: 175, Train Loss: -0.947942324479421, Validation Loss: -0.47935742139816284
Epoch: 176, Train Loss: -0.9614133742120531, Validation Loss: -0.47674915194511414
Epoch: 177, Train Loss: -0.9551278842820061, Validation Loss: -0.4853723645210266
Epoch: 178, Train Loss: -0.9643822484546237, Validation Loss: -0.4793822467327118
Epoch: 179, Train Loss: -0.969389373726315, Validation Loss: -0.4837295114994049
Epoch: 180, Train Loss: -0.9157331930266486, Validation Loss: -0.46184632182121277
Epoch: 181, Train Loss: -0.9355471902423435, Validation Loss: -0.4802212417125702
Epoch: 182, Train Loss: -0.9611553205384149, Validation Loss: -0.48990505933761597
Epoch: 183, Train Loss: -0.9352145936754015, Validation Loss: -0.45011308789253235
Epoch: 184, Train Loss: -0.9374565773540073, Validation Loss: -0.4754351079463959
Epoch: 185, Train Loss: -0.9291616704728868, Validation Loss: -0.4534876048564911
Epoch: 186, Train Loss: -0.9562710152732001, Validation Loss: -0.4826403558254242
Epoch: 187, Train Loss: -0.9459387567308214, Validation Loss: -0.46878933906555176
Epoch: 188, Train Loss: -0.9463278585010104, Validation Loss: -0.48188379406929016
Epoch: 189, Train Loss: -0.9557227863205804, Validation Loss: -0.47933053970336914
Epoch: 190, Train Loss: -0.9715407583448622, Validation Loss: -0.4767415225505829

